{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MemoryProfiling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tonycccccc/tony.github.io/blob/main/MemoryProfiling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QENBGjMov0U3"
      },
      "source": [
        "Notebook for doing MemoryProfiling -- All the attention layer related matrices in this notebook are generated through running target githuib repo. \\\n",
        "Author: Zeyu Chen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8DAsG7P3czW"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1DhEOnv9Sl0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ff05471-df99-4006-9507-1a36ec453849"
      },
      "source": [
        "! nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Dec 14 16:55:39 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHQztIIDaam7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4a642d8-e11c-4807-f15d-f31e8aacffee"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmVJeDXrvYtz"
      },
      "source": [
        "# Memory Profiling Sample Method\n",
        "def MemoryCheck(self, memory_used):\n",
        "if (tf.config.list_physical_devices('GPU')):\n",
        "  # Reser the memory state\n",
        "  tf.config.experimental.reset_memory_stats('GPU:0')\n",
        "  # Creates the first peak memory usage.\n",
        "  x = tf.convert_to_tensor(memory_used)\n",
        "  del x\n",
        "  memory = tf.config.experimental.get_memory_info('GPU:0')['peak']\n",
        "  return memory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sefKcjsmpGmP"
      },
      "source": [
        "# Iteration times we want to test our algorithm\n",
        "\n",
        "iteration_num = 12\n",
        "matrix_num = 12"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNhbXashaT5v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7cd0f6b-279e-4a4f-8ed5-36ee4ed0aac2"
      },
      "source": [
        "import glob\n",
        "import os\n",
        "# load matrix from files\n",
        "all_query_files = glob.glob(\"/content/drive/MyDrive/models/transformer/logging_query*.txt\")\n",
        "all_key_files = glob.glob(\"/content/drive/MyDrive/models/transformer/logging_key*.txt\")\n",
        "all_value_files = glob.glob(\"/content/drive/MyDrive/models/transformer/logging_value*.txt\")\n",
        "all_bias_files = glob.glob(\"/content/drive/MyDrive/models/transformer/logging_bias*.txt\")\n",
        "query_matrix = []\n",
        "key_matrix = []\n",
        "value_matrix = []\n",
        "for file in all_query_files:\n",
        "    query_file = tf.io.read_file(file)\n",
        "    query = tf.io.parse_tensor(query_file, out_type=tf.float32)\n",
        "    query_matrix.append(query)\n",
        "for file in all_key_files:\n",
        "    key_file = tf.io.read_file(file)\n",
        "    key = tf.io.parse_tensor(key_file, out_type=tf.float32)\n",
        "    key_matrix.append(key)\n",
        "for file in all_value_files:\n",
        "    value_file = tf.io.read_file(file)\n",
        "    value = tf.io.parse_tensor(value_file, out_type=tf.float32)\n",
        "    value_matrix.append(value)\n",
        "query_matrix[0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 64, 16, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXzk58Y0Qb9T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28e6d30d-3853-4bd3-afca-39aea73d13d8"
      },
      "source": [
        "# Testing matrix with all one large matrixes\n",
        "# tf.concat not doing same thing\n",
        "# All three matrix at this time should have shape as (64k) * 64 * 16 * 64\n",
        "query_matrix = np.concatenate(query_matrix)\n",
        "key_matrix = np.concatenate(key_matrix)\n",
        "value_matrix = np.concatenate(value_matrix)\n",
        "print(query_matrix.shape)\n",
        "print(key_matrix.shape)\n",
        "print(value_matrix.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(576, 64, 16, 64)\n",
            "(576, 64, 16, 64)\n",
            "(576, 64, 16, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_XYCEKdN7Rg"
      },
      "source": [
        "# Testing matrix mixed with large and normal matrix\n",
        "query_matrix = np.random.shuffle(query_matrix)\n",
        "prob = np.random.rand(1)\n",
        "data = []\n",
        "for i in range(query_matrix.size):\n",
        "  if prob > 0.5:\n",
        "    num = np.random.randint(10)\n",
        "    matrix = np.random.choice(query_matrix, [num, 1])\n",
        "    data.append(np.concatenate(matrix))\n",
        "  else:\n",
        "    data.append(np.random.choice(query_matrix, 1))\n",
        "query_matrix = np.array(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvSsVBEYUdYn"
      },
      "source": [
        "# Testing with several randomly generated large matrix\n",
        "data_query = []\n",
        "data_key = []\n",
        "data_value = []\n",
        "np.random.shuffle(query_matrix)\n",
        "np.random.shuffle(key_matrix)\n",
        "np.random.shuffle(value_matrix)\n",
        "for i in range(iteration_num):\n",
        "  if i == iteration_num // 2:\n",
        "    np.random.shuffle(query_matrix)\n",
        "    np.random.shuffle(key_matrix)\n",
        "    np.random.shuffle(value_matrix)\n",
        "  data_query.append(np.concatenate(query_matrix))\n",
        "  data_key.append(np.concatenate(key_matrix))\n",
        "  data_value.append(np.concatenate(value_matrix))\n",
        "  # data_query.append(query_matrix)\n",
        "  # data_key.append(key_matrix)\n",
        "  # data_value.append(value_matrix)\n",
        "# query_matrix = np.concatenate(data_query)[None, :, :, :, :]\n",
        "# key_matrix = np.concatenate(data_key)[None, :, :, :, :]\n",
        "# value_matrix = np.concatenate(data_value)[None, :, :, :, :]\n",
        "# query_matrix = np.concatenate(data_query)\n",
        "# key_matrix = np.concatenate(data_key)\n",
        "# value_matrix = np.concatenate(data_value)\n",
        "query_matrix = np.array(data_query)\n",
        "key_matrix = np.array(data_key)\n",
        "value_matrix = np.array(data_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njnliF8CJ1u4"
      },
      "source": [
        "# Generate Test Matrix with shape 1 * n * 16 * 64\n",
        "# Set matrix_size to 2k, 4k, 8k, 16k, 32k, and 64k\n",
        "matrix_size = 32 * 1024\n",
        "data_query = []\n",
        "data_key = []\n",
        "data_value = []\n",
        "for i in range(matrix_size // 64):\n",
        "  idx = np.random.randint(0, 576)\n",
        "  data_query.append(query_matrix[idx, :, :, :])\n",
        "  data_key.append(key_matrix[idx, :, :, :])\n",
        "  data_value.append(value_matrix[idx, :, :, :])\n",
        "query_matrix = np.concatenate(data_query)[None, :, :, :]\n",
        "key_matrix = np.concatenate(data_key)[None, :, :, :]\n",
        "value_matrix = np.concatenate(data_value)[None, :, :, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_matrix.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yu7u75g-05qf",
        "outputId": "b161cf47-25f8-4133-aeb0-9c818d3022da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12, 576, 64, 16, 64)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyPs4e8yU1Ef"
      },
      "source": [
        "# bias matrix are generated randomly with undetermined shape in the original code repo, so process it separately here\n",
        "dummy = []\n",
        "for file in all_bias_files:\n",
        "    bias_file = tf.io.read_file(file)\n",
        "    bias = tf.io.parse_tensor(bias_file, out_type=tf.float32)\n",
        "    dummy = dummy + list(bias.numpy().reshape([-1]))\n",
        "dummy = np.array(dummy)\n",
        "#idx = np.random.choice(np.array(dummy), tuple(query_matrix.shape), replace=True)\n",
        "idx = np.random.choice(np.array(dummy), [1, 64, 1, 64], replace=True)\n",
        "bias_matrix = idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WG5PC28O2WKl"
      },
      "source": [
        "# Convert all numpy matrices to tensors before doing the memory profiling\n",
        "query_matrix = tf.convert_to_tensor(query_matrix)\n",
        "key_matrix = tf.convert_to_tensor(key_matrix)\n",
        "value_matrix = tf.convert_to_tensor(value_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vFKA8GWnCLY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53087218-1b8b-46b8-e1c4-506749be92b6"
      },
      "source": [
        "# Append all profiling result to a csv file so we can do data visualization\n",
        "old_file_path = \"/content/drive/MyDrive/models/transformer/old_time_result.txt\"\n",
        "if False:\n",
        "  os.remove(old_file_path)\n",
        "f = open(old_file_path, 'a')\n",
        "f.write(\"\\nTest with one matrix with 576 * 64 * 16 * 64 shape\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "52"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEWnQ8_jVRq1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6db885d-154c-476e-9c9e-f42adb0422c2"
      },
      "source": [
        "import time\n",
        "# Code with separated test matrix\n",
        "res = []\n",
        "if (tf.config.list_physical_devices('GPU')):\n",
        "  tf.keras.backend.clear_session()\n",
        "  print(\"Before doing attention operations. Reset the memory\")\n",
        "  tf.config.experimental.reset_memory_stats('GPU:0')\n",
        "  memory1 = tf.config.experimental.get_memory_info('GPU:0')\n",
        "  print(memory1['peak'])\n",
        "  print(memory1['current'])\n",
        "  # f.write(\"Before running : (%f, %f)\\n\" % (memory1['peak'], memory1['current']))\n",
        "  start = time.time()\n",
        "  for i in range(matrix_num):\n",
        "    query = query_matrix[i]\n",
        "    key = key_matrix[i]\n",
        "    value = value_matrix[i]\n",
        "    # query = query_matrix\n",
        "    # key = key_matrix\n",
        "    # value = value_matrix\n",
        "    bias_value = bias_matrix\n",
        "    bias = bias_value[0, 0, 0, 0]\n",
        "    logits = tf.einsum(\"BTNH,BFNH->BNFT\", key, query)\n",
        "    logits += bias\n",
        "    weights = tf.nn.softmax(logits, name=\"attention_weights\")\n",
        "    weights = tf.nn.dropout(weights, rate=0.4)\n",
        "    old_attention_output = tf.einsum(\"BNFT,BTNH->BFNH\", weights, value)\n",
        "    res.append(old_attention_output)\n",
        "    memory2 = tf.config.experimental.get_memory_info('GPU:0')\n",
        "    print(\"Memory after attention operation.\")\n",
        "    print(memory2['peak'])\n",
        "    print(memory2['current'])\n",
        "    #f.write(\"After Iteration %d : (%f, %f)\\n\" % (i, memory2['peak'], memory2['current']))\n",
        "    f.write(\"Iteration %d : time %f\" % (i, time.time()-start))\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before doing attention operations. Reset the memory\n",
            "5435819264\n",
            "5435819264\n",
            "Memory after attention operation.\n",
            "6446647040\n",
            "5888804608\n",
            "Memory after attention operation.\n",
            "6635390720\n",
            "6073353984\n",
            "Memory after attention operation.\n",
            "6824134400\n",
            "6224348928\n",
            "Memory after attention operation.\n",
            "7012878080\n",
            "6375343872\n",
            "Memory after attention operation.\n",
            "7050626816\n",
            "6492784384\n",
            "Memory after attention operation.\n",
            "7541360384\n",
            "6979323648\n",
            "Memory after attention operation.\n",
            "7692355328\n",
            "7130318592\n",
            "Memory after attention operation.\n",
            "7843350272\n",
            "7281313536\n",
            "Memory after attention operation.\n",
            "7994345216\n",
            "7432308480\n",
            "Memory after attention operation.\n",
            "8145340160\n",
            "7583303424\n",
            "Memory after attention operation.\n",
            "8296335104\n",
            "7734298368\n",
            "Memory after attention operation.\n",
            "8485078784\n",
            "7923042048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeGRJ0TzqUtd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f67ef4fb-9333-47c8-9b4c-ecd4f1c8040d"
      },
      "source": [
        "new_file_path = \"/content/drive/MyDrive/models/transformer/newtime_result.txt\"\n",
        "if False:\n",
        "  os.remove(new_file_path)\n",
        "f = open(new_file_path, 'a')\n",
        "f.write(\"\\nTest with one matrix with 576 * 64 * 16 * 64 shape\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "52"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGKB066saArq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd0c5fd6-aae5-40ad-8032-96f753ba8ff6"
      },
      "source": [
        "import time\n",
        "# Try some new stuff\n",
        "new_res = []\n",
        "if (tf.config.list_physical_devices('GPU')):\n",
        "  tf.keras.backend.clear_session()\n",
        "  print(\"Before doing attention operations. Reset the memory\")\n",
        "  tf.config.experimental.reset_memory_stats('GPU:0')\n",
        "  memory1 = tf.config.experimental.get_memory_info('GPU:0')\n",
        "  print(memory1['peak'])\n",
        "  print(memory1['current'])\n",
        "  start = time.time()\n",
        "  # f.write(\"Before running : (%f, %f)\\n\" % (memory1['peak'], memory1['current']))\n",
        "  # 将变量整合到function里面，尝试融合多个value矩阵concat形成的大矩阵， 尝试更高的granularity\n",
        "  for i in range(matrix_num):\n",
        "    query = query_matrix[i]\n",
        "    key = key_matrix[i]\n",
        "    value = value_matrix[i]\n",
        "    # query = query_matrix\n",
        "    # key = key_matrix\n",
        "    # value = value_matrix\n",
        "    batch_size, source_length, head_num, dim = tf.shape(query).numpy()\n",
        "    _,key_length,_,_, = tf.shape(key).numpy()\n",
        "    batch_granularity = 32\n",
        "    head_granularity = 8\n",
        "    bias_value = bias_matrix[0, np.random.randint(64), 0, np.random.randint(64)]\n",
        "    # The outermost loop loops through batch_size, with granularity as stride\n",
        "    for batch in tf.range(0, batch_size, batch_granularity):\n",
        "      for head in tf.range(0, head_num, head_granularity):\n",
        "        batch_termination = batch + batch_granularity if batch + batch_granularity <= batch_size else batch_size\n",
        "        for unit_batch in tf.range(batch, batch_termination):\n",
        "          head_termination = head + head_granularity if head + head_granularity <= head_num else head_num\n",
        "          for unit_head in tf.range(head, head_termination):\n",
        "            for query_source in tf.split(query[unit_batch, :, unit_head, :], num_or_size_splits=1, axis=0):\n",
        "              key_source = key[unit_batch, :, unit_head, :]\n",
        "              result = tf.matmul(query_source, tf.transpose(key_source))\n",
        "              result = bias_value + result\n",
        "              #Row granularity\n",
        "              result = tf.nn.softmax(result, name=\"attention_weights\")\n",
        "              result = tf.nn.dropout(result, rate=0.4)\n",
        "            value_source = value[unit_batch, :, unit_head, :]\n",
        "            attention_output = tf.matmul(result, value_source) #Matrix with size F * H\n",
        "\n",
        "            if unit_head == head:\n",
        "              attention_output = tf.expand_dims(attention_output, axis=1)\n",
        "              output_from_unit_head = attention_output\n",
        "            else:\n",
        "              attention_output = tf.expand_dims(attention_output, axis=1)\n",
        "              output_from_unit_head = tf.concat([output_from_unit_head, attention_output], 1)\n",
        "          if unit_batch == batch:\n",
        "            output_from_unit_head = tf.expand_dims(output_from_unit_head, axis=0)\n",
        "            output_from_unit_batch = output_from_unit_head\n",
        "          else:\n",
        "            output_from_unit_head = tf.expand_dims(output_from_unit_head, axis=0)\n",
        "            output_from_unit_batch = tf.concat([output_from_unit_batch, output_from_unit_head], 0)\n",
        "        if head == 0:\n",
        "          suboutput = output_from_unit_batch\n",
        "        else:\n",
        "          suboutput = tf.concat([suboutput, output_from_unit_batch], 2)\n",
        "      if batch == 0:\n",
        "        output = suboutput\n",
        "      else:\n",
        "        output = tf.concat([output, suboutput], 0)\n",
        "    new_res.append(output)\n",
        "    memory2 = tf.config.experimental.get_memory_info('GPU:0')\n",
        "    print(\"Memory profiling using FLAT\")\n",
        "    print(memory2['peak'])\n",
        "    print(memory2['current'])\n",
        "    # f.write(\"After Iteration %d : (%f, %f)\\n\" % (i, memory2['peak'], memory2['current']))\n",
        "    f.write(\"After Iteration %d : (%f)\\n\" % (i, time.time() - start))\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before doing attention operations. Reset the memory\n",
            "5435819264\n",
            "5435819264\n",
            "Memory profiling using FLAT\n",
            "5774075648\n",
            "5774075648\n",
            "Memory profiling using FLAT\n",
            "5918676736\n",
            "5913999104\n",
            "Memory profiling using FLAT\n",
            "6118516480\n",
            "6066730752\n",
            "Memory profiling using FLAT\n",
            "6247065344\n",
            "6247028480\n",
            "Memory profiling using FLAT\n",
            "6467192576\n",
            "6467192576\n",
            "Memory profiling using FLAT\n",
            "6626645760\n",
            "6626645760\n",
            "Memory profiling using FLAT\n",
            "6766069504\n",
            "6764517120\n",
            "Memory profiling using FLAT\n",
            "6992389632\n",
            "6992389632\n",
            "Memory profiling using FLAT\n",
            "7131326208\n",
            "7131326208\n",
            "Memory profiling using FLAT\n",
            "7349475072\n",
            "7349475072\n",
            "Memory profiling using FLAT\n",
            "7492183808\n",
            "7492183808\n",
            "Memory profiling using FLAT\n",
            "7702025984\n",
            "7702025984\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qdyVxJw8usB",
        "outputId": "f7bee2e8-04d8-4d1a-d71c-45cb7218d7d8"
      },
      "source": [
        "! nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Dec  1 03:04:30 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    33W / 250W |  15855MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THDi9W77KW_w",
        "outputId": "f9c19175-4448-4081-e624-e6e8a61ce49f"
      },
      "source": [
        "optimized_file_path = \"/content/drive/MyDrive/models/transformer/optimizedtime_result.txt\"\n",
        "if False:\n",
        "  os.remove(new_file_path)\n",
        "f = open(optimized_file_path, 'a')\n",
        "f.write(\"\\nTest with 12 matrix with each 576 * 64 * 16 * 64 shape\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "56"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jOX-tIPYqcs",
        "outputId": "c879f416-d8d0-465d-db7f-758deaabd955"
      },
      "source": [
        "print(query_matrix.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 16384, 16, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYw7HYAhKiSC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75a1441a-30c2-4a1c-e3de-4a93ff9eea2c"
      },
      "source": [
        "import time\n",
        "# What if user wants to try even higher granularity?\n",
        "new_res = []\n",
        "if (tf.config.list_physical_devices('GPU')):\n",
        "  # tf.keras.backend.clear_session()\n",
        "  print(\"Before doing attention operations. Reset the memory\")\n",
        "  tf.config.experimental.reset_memory_stats('GPU:0')\n",
        "  memory1 = tf.config.experimental.get_memory_info('GPU:0')\n",
        "  print(memory1['peak'])\n",
        "  print(memory1['current'])\n",
        "  f.write(\"Before running : (%f, %f)\\n\" % (memory1['peak'], memory1['current']))\n",
        "  start = time.time()\n",
        "  for i in range(matrix_num):\n",
        "    qeury = query_matrix[i]\n",
        "    key = key_matrix[i]\n",
        "    value = value_matrix[i]\n",
        "    # query = query_matrix\n",
        "    # key = key_matrix\n",
        "    # value = value_matrix\n",
        "    batch_size, source_length, head_num, dim = tf.shape(query).numpy()\n",
        "    _,key_length,_,_, = tf.shape(key).numpy()\n",
        "    batch_granularity = 32\n",
        "    head_granularity = 16\n",
        "    bias_value = bias_matrix[0, np.random.randint(64), 0, np.random.randint(64)]\n",
        "    for batch in tf.range(0, batch_size, batch_granularity):\n",
        "      for head in tf.range(0, head_num, head_granularity):\n",
        "        batch_termination = batch + batch_granularity if batch + batch_granularity <= batch_size else batch_size\n",
        "        for unit_batch in tf.range(batch, batch_termination):\n",
        "          head_termination = head + head_granularity if head + head_granularity <= head_num else head_num\n",
        "          for unit_head in tf.range(head, head_termination, 8):\n",
        "            # query_source now should be 64 * 8 * 64\n",
        "            query_source = tf.gather(query[unit_batch, :, :, :], indices=tf.range(unit_head, unit_head + 8), axis=1)\n",
        "            key_source = tf.gather(key[unit_batch, :, :, :], indices=tf.range(unit_head, unit_head + 8), axis=1)\n",
        "            result = tf.einsum(\"TNH, FNH->NFT\", key_source, query_source)\n",
        "            # result += bias_value[None, :, :]\n",
        "            result += bias_value\n",
        "            result = tf.nn.softmax(result, name=\"attention_weights\")\n",
        "            result = tf.nn.dropout(result, rate=0.4)\n",
        "            if unit_head == head:\n",
        "              logit = result\n",
        "            else:\n",
        "              # Concatenate over head num dimension\n",
        "              logit = tf.concat([logit, result], axis=0)\n",
        "          value_source = value[unit_batch, :, :, :]\n",
        "          attention_output = tf.einsum(\"NFT,TNH->FNH\", logit, value_source)\n",
        "          if unit_batch == batch:\n",
        "            attention_output = tf.expand_dims(attention_output, axis=0)\n",
        "            output_from_unit_batch = attention_output\n",
        "          else:\n",
        "            attention_output = tf.expand_dims(attention_output, axis=0)\n",
        "            output_from_unit_batch = tf.concat([output_from_unit_batch, attention_output], 0)\n",
        "        if head == 0:\n",
        "          suboutput = output_from_unit_batch\n",
        "        else:\n",
        "          suboutput = tf.concat([suboutput, output_from_unit_batch], 2)\n",
        "      if batch == 0:\n",
        "        output = suboutput\n",
        "      else:\n",
        "        output = tf.concat([output, suboutput], 0)\n",
        "    new_res.append(output)\n",
        "    memory2 = tf.config.experimental.get_memory_info('GPU:0')\n",
        "    print(\"Memory profiling using FLAT\")\n",
        "    print(memory2['peak'])\n",
        "    print(memory2['current'])\n",
        "    f.write(\"After Iteration %d : (%f)\\n\" % (i, time.time()-start))\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before doing attention operations. Reset the memory\n",
            "5435819264\n",
            "5435819264\n",
            "Memory profiling using FLAT\n",
            "5501091584\n",
            "5501091584\n",
            "Memory profiling using FLAT\n",
            "5517639424\n",
            "5517639424\n",
            "Memory profiling using FLAT\n",
            "5538381824\n",
            "5534615040\n",
            "Memory profiling using FLAT\n",
            "5551195648\n",
            "5551195648\n",
            "Memory profiling using FLAT\n",
            "5571280896\n",
            "5571280896\n",
            "Memory profiling using FLAT\n",
            "5594744320\n",
            "5594744320\n",
            "Memory profiling using FLAT\n",
            "5611388672\n",
            "5611388672\n",
            "Memory profiling using FLAT\n",
            "5628821248\n",
            "5628821248\n",
            "Memory profiling using FLAT\n",
            "5645565696\n",
            "5645565696\n",
            "Memory profiling using FLAT\n",
            "5664997120\n",
            "5661984256\n",
            "Memory profiling using FLAT\n",
            "5684002816\n",
            "5681316096\n",
            "Memory profiling using FLAT\n",
            "5707104512\n",
            "5698357248\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kntxUU2JxLgh"
      },
      "source": [
        "# Check Result\n",
        "diff = np.abs((np.concatenate(res) - np.concatenate(new_res))) < 1\n",
        "np.all(diff)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Y79LsNtEg5a"
      },
      "source": [
        "output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYlGMXOba90J",
        "outputId": "42bc764a-f501-4cc9-92c0-7276073be824"
      },
      "source": [
        "old_attention_output = tf.cast(old_attention_output, dtype=tf.float64)\n",
        "old_attention_output.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([32, 32, 2, 10])"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSLnTX2gxNZG"
      },
      "source": [
        "np.abs(output - old_attention_output) < 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUXvdSQK3WkS"
      },
      "source": [
        "1. GPU timing\n",
        "2. visualization\n",
        "3. Compress to one function\n",
        "4. Powerpoint\n",
        "5. Github page with a UI"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
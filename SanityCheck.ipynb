{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SanityCheck.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hbruBMEtHkr"
      },
      "source": [
        "This notebook is for doing SanityCheck of the FLAT dataflow. All data in this file are randomly generated. \\\\\n",
        "Author: Zeyu Chen\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYAGcgXks4UJ"
      },
      "source": [
        "# Import library\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tknFhuetj_9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63521aee-73d5-4790-b379-a0086a79f82b"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDlEYKLYtoJy"
      },
      "source": [
        "\"\"\"\n",
        "Unit Test Test Data\n",
        "\"\"\"\n",
        "# Set a random query_input with size 32*32*2*10 Batch_size * source_length * num_head * dim_per_head\n",
        "query_input = np.ones([64, 64, 16, 64])\n",
        "temp = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16])[None, None, :, None]\n",
        "query_input= query_input * temp\n",
        "query_input\n",
        "# Create a random key_input with size 32 * 64 * 2 * 10 Batch_size * query_length * num_head * dim_per_head\n",
        "key_input = np.ones([64, 64, 16, 64])\n",
        "temp = np.array([10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160])[None, None, :, None]\n",
        "key_input= key_input * temp\n",
        "key_input\n",
        "# Create random bias\n",
        "bias = np.random.random([64, 16, 64, 64])\n",
        "bias.shape\n",
        "# Create random value matrix with size 32 * 64 * 2 * 10 Batch_size * length * head_num * dim_per_num\n",
        "values = np.random.randint(0, 100, [64, 64, 16, 64])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJxvW4pgt_E0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "898211f2-5717-47ed-dd3f-61ffe262cab1"
      },
      "source": [
        "# Create logit output\n",
        "logits = tf.einsum(\"BTNH,BFNH->BNFT\", key_input, query_input)\n",
        "# Add logits with bias\n",
        "logits += bias[0, 0, :, :]\n",
        "# Pass through the softmax layer\n",
        "weights = tf.nn.softmax(logits, name=\"attention_weights\")\n",
        "# Pass through the dropout\n",
        "#weights = tf.nn.dropout(weights, rate=0.4)\n",
        "# Original code attention output\n",
        "old_attention_output = tf.einsum(\"BNFT,BTNH->BFNH\", weights, values)\n",
        "old_attention_output.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 64, 16, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnVsAvSWucaY"
      },
      "source": [
        "# Prepare data for testing\n",
        "query = tf.convert_to_tensor(query_input)\n",
        "key = tf.convert_to_tensor(key_input)\n",
        "bias = tf.convert_to_tensor(bias)\n",
        "values = tf.convert_to_tensor(values)\n",
        "\n",
        "\n",
        "# FLAT Implementation   ---  First Version\n",
        "#@title First Implement Version\n",
        "\"\"\"Fuse Logit and Attend operators\n",
        "\n",
        "Args:\n",
        "  query_input: query output from query_dense_layer with shape [batch_size, length, num_heads, dim_per_head].\n",
        "  key_input: key output from key_dense_layer with shape [batch_size, length, num_heads, dim_per_head].\n",
        "  value_input: value output from value_dense_layer with shape [batch_size, length, num_heads, dim_per_head].\n",
        "  bias_matrix: A tensor with shape [batch_size, 1, length_query, length_source],\n",
        "\n",
        "Symbol meanings:\n",
        "  B : batch size\n",
        "  T : key_length == length_qeury\n",
        "  N : Head_num\n",
        "  H : dim_per_head\n",
        "  F : query_length == length_source (which also commented as )\n",
        "Returns:\n",
        "  Attention layer output with shape [batch_size, length_query, hidden_size]\n",
        "  logits = tf.einsum(\"BTNH,BFNH->BNFT\", key, query)\n",
        "  attention_output = tf.einsum(\"BNFT,BTNH->BFNH\", weights, value)\n",
        "\"\"\"\n",
        "# Randomly set granularity now\n",
        "batch_granularity = 1\n",
        "head_granularity = 1\n",
        "element_granularity = 1\n",
        "\n",
        "# Get shape of the matrix\n",
        "batch_size, source_length, head_num, dim = query.shape.as_list()\n",
        "_,key_length,_,_ = key.shape.as_list()\n",
        "output = None\n",
        "assert (head_granularity < head_num)\n",
        "\n",
        "# The outermost loop loops through batch_size, with granularity as stride\n",
        "for batch in range(0, batch_size, batch_granularity):\n",
        "  flag = True\n",
        "  suboutput = None\n",
        "  # The inner loop loops through head, with granularity as stride\n",
        "  for head in range(0, head_num, head_granularity):\n",
        "    #Head_Dimension = len(tf.unstack(query, axis = 3))\n",
        "    #for element in range(0, Head_Dimension, element_granularity):\n",
        "      #for loop in range(1):\n",
        "    output_from_unit_batch = np.zeros([batch_granularity, source_length, head_granularity, dim])\n",
        "    # Check each bach inside each granularity\n",
        "    for unit_batch in range(batch, batch + batch_granularity):\n",
        "      output_from_unit_head = np.zeros([source_length, head_granularity, dim])\n",
        "      # Check each head in the head granularity\n",
        "      for unit_head in range(head, head + head_granularity):\n",
        "        #for unit_element in range(element):\n",
        "        logit_output = np.ones([source_length, key_length])\n",
        "        for loop in range(2):\n",
        "          if loop == 0:\n",
        "            #iterate through query_length\n",
        "            for unit_source in range(source_length):\n",
        "              query_source = query_input[unit_batch, unit_source, unit_head, :]\n",
        "              query_source = tf.reshape(query_source, [1, query_source.size]) # Tensor with 1 * 10 (unit_source_length * dim_per_head)\n",
        "              key_source = key[unit_batch, :, unit_head, :] # Tensor with 64 * 10 (key length * dim_per_head)\n",
        "              #Result is F(1) * T matrix\n",
        "              #Not sure if we need to do transpose here\n",
        "              #result = tf.matmul(query_source, key_source) # Tensor with 1 * 64 size (unit_source_length * key_length)\n",
        "              result = tf.matmul(query_source, tf.transpose(key_source)) # Tensor with 1 * 64 size (unit_source_length * key_length)\n",
        "              result += bias[unit_batch, :, unit_source, :]\n",
        "              #Row granularity\n",
        "              result = tf.nn.softmax(result, name=\"attention_weights\", axis=-1)\n",
        "              result = tf.nn.dropout(result, rate=0.4)\n",
        "              logit_output[unit_source, :] = result\n",
        "            #Logit output now is F * T matrix\n",
        "            logit_output = tf.convert_to_tensor(logit_output)\n",
        "          else:\n",
        "            value_source = values[unit_batch, :, unit_head, :] # Tensor with 64 * 10 (T * H) query_length * dim_per_head\n",
        "            logit_output = tf.cast(logit_output, dtype=float)\n",
        "            value_source = tf.cast(value_source, dtype=float)\n",
        "            attention_output = tf.matmul(logit_output, value_source) #Matrix with size F * H\n",
        "        output_from_unit_head[:, unit_head - head, :] = attention_output\n",
        "      #Size now is F*N*H (output_from_unit_head)\n",
        "      output_from_unit_batch[unit_batch - batch, :, :, :] = output_from_unit_head\n",
        "    # Size now is BFNH (output from unit batch)\n",
        "    if flag == True:\n",
        "      suboutput = output_from_unit_batch\n",
        "      flag = False\n",
        "    else:\n",
        "      suboutput = tf.concat([suboutput, output_from_unit_batch], 2)\n",
        "  if output == None:\n",
        "    output = suboutput\n",
        "  else:\n",
        "    output = tf.concat([output, suboutput], 0)\n",
        "print(output.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9sh3cJeuw1p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96a28940-55f5-4e45-9541-46a059fa4967"
      },
      "source": [
        "# Optimized Version\n",
        "# 加入dropout层后，结果便无法比较\n",
        "batch_size, source_length, head_num, dim = tf.shape(query_input).numpy()\n",
        "_,key_length,_,_, = tf.shape(key_input).numpy()\n",
        "batch_granularity = 32\n",
        "head_granularity = 8\n",
        "# Broadcasting to reshape the bias matrix to the shape as the logit BNFT\n",
        "# The outermost loop loops through batch_size, with granularity as stride\n",
        "for batch in tf.range(0, batch_size, batch_granularity):\n",
        "  for head in tf.range(0, head_num, head_granularity):\n",
        "    # Next Line removed now since input size is fixed and the result is evenly divided\n",
        "    batch_termination = batch + batch_granularity if batch + batch_granularity <= batch_size else batch_size\n",
        "    for unit_batch in tf.range(batch, batch_termination):\n",
        "      # Next Line removed now since input size is fixed and the result is evenly divided\n",
        "      head_termination = head + head_granularity if head + head_granularity <= head_num else head_num\n",
        "      for unit_head in tf.range(head, head + head_granularity):\n",
        "        for loop in tf.range(2):\n",
        "          if loop == 0:\n",
        "            # Tensorflow not support multiple indexing\n",
        "            for query_source in tf.split(query_input[unit_batch, :, unit_head, :], num_or_size_splits=1, axis=0):\n",
        "              key_source = key_input[unit_batch, :, unit_head, :]\n",
        "              result = tf.matmul(query_source, tf.transpose(key_source))\n",
        "              result += bias[unit_batch, unit_head, :, :]\n",
        "              #Row granularity\n",
        "              result = tf.nn.softmax(result, name=\"attention_weights\")\n",
        "              #result = tf.nn.dropout(result, rate=0.4)\n",
        "              #logit_output = result\n",
        "          else:\n",
        "            value_source = values[unit_batch, :, unit_head, :] \n",
        "            attention_output = tf.matmul(result, value_source) #Matrix with size F * H\n",
        "        if unit_head == head:\n",
        "          attention_output = tf.expand_dims(attention_output, axis=1)\n",
        "          output_from_unit_head = attention_output\n",
        "        else:\n",
        "          attention_output = tf.expand_dims(attention_output, axis=1)\n",
        "          output_from_unit_head = tf.concat([output_from_unit_head, attention_output], 1)\n",
        "      if unit_batch == batch:\n",
        "        output_from_unit_head = tf.expand_dims(output_from_unit_head, axis=0)\n",
        "        output_from_unit_batch = output_from_unit_head\n",
        "      else:\n",
        "        output_from_unit_head = tf.expand_dims(output_from_unit_head, axis=0)\n",
        "        output_from_unit_batch = tf.concat([output_from_unit_batch, output_from_unit_head], 0)\n",
        "    if head == 0:\n",
        "      suboutput = output_from_unit_batch\n",
        "    else:\n",
        "      suboutput = tf.concat([suboutput, output_from_unit_batch], 2)\n",
        "  if batch == 0:\n",
        "    output = suboutput\n",
        "  else:\n",
        "    output = tf.concat([output, suboutput], 0)\n",
        "output.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 64, 16, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFElGPvqsSuO",
        "outputId": "a6ae577c-1560-49b3-c4bf-e5979103bc3e"
      },
      "source": [
        "old_attention_output = tf.cast(old_attention_output, tf.float64)\n",
        "np.all(np.abs(old_attention_output - output) < 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hue87dAU8QfE"
      },
      "source": [
        "# What if user wants to try even higher granularity?\n",
        "# new_res = []\n",
        "# if (tf.config.list_physical_devices('GPU')):\n",
        "#   tf.keras.backend.clear_session()\n",
        "#   print(\"Before doing attention operations. Reset the memory\")\n",
        "#   tf.config.experimental.reset_memory_stats('GPU:0')\n",
        "#   memory1 = tf.config.experimental.get_memory_info('GPU:0')\n",
        "#   print(memory1['peak'])\n",
        "#   print(memory1['current'])\n",
        "#   f.write(\"Before running : (%f, %f)\\n\" % (memory1['peak'], memory1['current']))\n",
        "#   for i in range(matrix_num):\n",
        "# qeury = query_matrix[i]\n",
        "# key = key_matrix[i]\n",
        "# value = value_matrix[i]\n",
        "batch_size, source_length, head_num, dim = tf.shape(query_input).numpy()\n",
        "_,key_length,_,_, = tf.shape(key_input).numpy()\n",
        "batch_granularity = 32\n",
        "head_granularity = 16\n",
        "bias_value = bias[0, 0, :, :]\n",
        "for batch in tf.range(0, batch_size, batch_granularity):\n",
        "  for head in tf.range(0, head_num, head_granularity):\n",
        "    for unit_batch in tf.range(batch, batch + batch_granularity):\n",
        "      for unit_head in tf.range(head, head + head_granularity, 8):\n",
        "        # query_source now should be 1 * 64 * 8 * 64\n",
        "        query_source = tf.gather(query_input[unit_batch, :, :, :], indices=tf.range(head, head + 8), axis=1)\n",
        "        key_source = tf.gather(key_input[unit_batch, :, :, :], indices=tf.range(head, head + 8), axis=1)\n",
        "        result = tf.einsum(\"TNH, FNH->NFT\", key_source, query_source)\n",
        "        result += bias_value[None, :, :]\n",
        "        result = tf.nn.softmax(result, name=\"attention_weights\")\n",
        "        #result = tf.nn.dropout(result, rate=0.4)\n",
        "        if unit_head == head:\n",
        "          logit = result\n",
        "        else:\n",
        "          # Concatenate over head num dimension\n",
        "          logit = tf.concat([logit, result], axis=0)\n",
        "      value_source = values[unit_batch, :, :, :]\n",
        "      attention_output = tf.einsum(\"NFT,TNH->FNH\", logit, value_source)\n",
        "      if unit_batch == batch:\n",
        "        attention_output = tf.expand_dims(attention_output, axis=0)\n",
        "        output_from_unit_batch = attention_output\n",
        "      else:\n",
        "        attention_output = tf.expand_dims(attention_output, axis=0)\n",
        "        output_from_unit_batch = tf.concat([output_from_unit_batch, attention_output], 0)\n",
        "    if head == 0:\n",
        "      suboutput = output_from_unit_batch\n",
        "    else:\n",
        "      suboutput = tf.concat([suboutput, output_from_unit_batch], 2)\n",
        "  if batch == 0:\n",
        "    output = suboutput\n",
        "  else:\n",
        "    output = tf.concat([output, suboutput], 0)\n",
        "    # new_res.append(output)\n",
        "    # memory2 = tf.config.experimental.get_memory_info('GPU:0')\n",
        "    # print(\"Memory profiling using FLAT\")\n",
        "    # print(memory2['peak'])\n",
        "    # print(memory2['current'])\n",
        "    # f.write(\"After Iteration %d : (%f, %f)\\n\" % (i, memory2['peak'], memory2['current']))\n",
        "# f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dq38scCNuQl4",
        "outputId": "c98684f6-10e4-4c49-e03f-f45543c964d9"
      },
      "source": [
        "old_attention_output = tf.cast(old_attention_output, tf.float64)\n",
        "np.all(np.abs(old_attention_output - output) < 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70l1VTHsv3ry"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
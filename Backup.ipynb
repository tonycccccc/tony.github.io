{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Backup.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMyongIOyMaF1/mY6ePpeqE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tonycccccc/tony.github.io/blob/main/Backup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gxq8pbtSu3xk"
      },
      "source": [
        "#@title Original First Version Code\n",
        "\"\"\"Fuse Logit and Attend operators\n",
        "\n",
        "    Args:\n",
        "      query_input: query output from query_dense_layer with shape [batch_size, length, num_heads, dim_per_head].\n",
        "      key_input: key output from key_dense_layer with shape [batch_size, length, num_heads, dim_per_head].\n",
        "      value_input: value output from value_dense_layer with shape [batch_size, length, num_heads, dim_per_head].\n",
        "      bias_matrix: A tensor with shape [batch_size, 1, length_query, length_source],\n",
        "\n",
        "    Symbol meanings:\n",
        "      B : batch size\n",
        "      T : key_length == length_qeury\n",
        "      N : Head_num\n",
        "      H : dim_per_head\n",
        "      F : query_length == length_source (which also commented as )\n",
        "    Returns:\n",
        "      Attention layer output with shape [batch_size, length_query, hidden_size]\n",
        "\n",
        "      logits = tf.einsum(\"BTNH,BFNH->BNFT\", key, query)\n",
        "      attention_output = tf.einsum(\"BNFT,BTNH->BFNH\", weights, value)\n",
        "\"\"\"\n",
        "# Randomly set granularity now\n",
        "batch_granularity = 1\n",
        "head_granularity = 1\n",
        "element_granularity = 1\n",
        "\n",
        "# Get shape of the matrix\n",
        "batch_size, source_length, head_num, dim = query.shape.as_list()\n",
        "_,key_length,_,_ = key.shape.as_list()\n",
        "output = None\n",
        "assert (head_granularity < head_num)\n",
        "\n",
        "# The outermost loop loops through batch_size, with granularity as stride\n",
        "for batch in range(0, batch_size, batch_granularity):\n",
        "  flag = True\n",
        "  suboutput = None\n",
        "  # The inner loop loops through head, with granularity as stride\n",
        "  for head in range(0, head_num, head_granularity):\n",
        "    #Head_Dimension = len(tf.unstack(query, axis = 3))\n",
        "    #for element in range(0, Head_Dimension, element_granularity):\n",
        "      #for loop in range(1):\n",
        "    output_from_unit_batch = np.zeros([batch_granularity, source_length, head_granularity, dim])\n",
        "    # Check each bach inside each granularity\n",
        "    for unit_batch in range(batch, batch + batch_granularity):\n",
        "      output_from_unit_head = np.zeros([source_length, head_granularity, dim])\n",
        "      # Check each head in the head granularity\n",
        "      for unit_head in range(head, head + head_granularity):\n",
        "        #for unit_element in range(element):\n",
        "        logit_output = np.ones([source_length, key_length])\n",
        "        for loop in range(2):\n",
        "          if loop == 0:\n",
        "            #iterate through query_length\n",
        "            for unit_source in range(source_length):\n",
        "              query_source = query_input[unit_batch, unit_source, unit_head, :]\n",
        "              query_source = tf.reshape(query_source, [1, query_source.size]) # Tensor with 1 * 10 (unit_source_length * dim_per_head)\n",
        "              key_source = key[unit_batch, :, unit_head, :] # Tensor with 64 * 10 (key length * dim_per_head)\n",
        "              #Result is F(1) * T matrix\n",
        "              #Not sure if we need to do transpose here\n",
        "              #result = tf.matmul(query_source, key_source) # Tensor with 1 * 64 size (unit_source_length * key_length)\n",
        "              result = tf.matmul(query_source, tf.transpose(key_source)) # Tensor with 1 * 64 size (unit_source_length * key_length)\n",
        "              result += bias[unit_batch, :, unit_source, :]\n",
        "              #Row granularity\n",
        "              result = tf.nn.softmax(result, name=\"attention_weights\", axis=-1)\n",
        "              logit_output[unit_source, :] = result\n",
        "            #Logit output now is F * T matrix\n",
        "            logit_output = tf.convert_to_tensor(logit_output)\n",
        "          else:\n",
        "            value_source = values[unit_batch, :, unit_head, :] # Tensor with 64 * 10 (T * H) query_length * dim_per_head\n",
        "            logit_output = tf.cast(logit_output, dtype=float)\n",
        "            value_source = tf.cast(value_source, dtype=float)\n",
        "            attention_output = tf.matmul(logit_output, value_source) #Matrix with size F * H\n",
        "        output_from_unit_head[:, unit_head - head, :] = attention_output\n",
        "      #Size now is F*N*H (output_from_unit_head)\n",
        "      output_from_unit_batch[unit_batch - batch, :, :, :] = output_from_unit_head\n",
        "    # Size now is BFNH (output from unit batch)\n",
        "    if flag == True:\n",
        "      suboutput = output_from_unit_batch\n",
        "      flag = False\n",
        "    else:\n",
        "      suboutput = tf.concat([suboutput, output_from_unit_batch], 2)\n",
        "  if output == None:\n",
        "    output = suboutput\n",
        "  else:\n",
        "    output = tf.concat([output, suboutput], 0)\n",
        "print(output.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Up5_g7NPu6Fr"
      },
      "source": [
        "\"\"\"\n",
        "Unit Test\n",
        "\"\"\"\n",
        "# Set a random query_input with size 32*32*2*10 Batch_size * source_length * num_head * dim_per_head\n",
        "query_input = np.ones([32, 32, 2, 10])\n",
        "temp = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
        "query_input= query_input * temp\n",
        "query_input\n",
        "# Create a random key_input with size 32 * 64 * 2 * 10 Batch_size * query_length * num_head * dim_per_head\n",
        "key_input = np.ones([32, 64, 2, 10])\n",
        "temp = np.array([10, 20, 30, 40, 50, 60, 70, 80, 90, 100])\n",
        "key_input= key_input * temp\n",
        "key_input\n",
        "# Create random bias\n",
        "bias = np.random.random([32, 1, 32, 64])\n",
        "bias.shape\n",
        "# Create random value matrix with size 32 * 64 * 2 * 10 Batch_size * length * head_num * dim_per_num\n",
        "values = np.random.randint(0, 100, [32, 64, 2, 10])\n",
        "values.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwApwOU61AY1"
      },
      "source": [
        "# Code with separated test matrix\n",
        "res = []\n",
        "if (tf.config.list_physical_devices('GPU')):\n",
        "  tf.config.experimental.reset_memory_stats('GPU:0')\n",
        "  memory = tf.config.experimental.get_memory_info('GPU:0')\n",
        "  print(\"Before doing attention operations. Reset the memory\")\n",
        "  print(memory['peak'])\n",
        "  print(memory['current'])\n",
        "  for i in range(len(query_matrix)):\n",
        "    query = query_matrix[i]\n",
        "    key = key_matrix[i]\n",
        "    bias = bias_matrix[i]\n",
        "    value = value_matrix[i]\n",
        "    logits = tf.einsum(\"BTNH,BFNH->BNFT\", key, query)\n",
        "    logits += bias\n",
        "    weights = tf.nn.softmax(logits, name=\"attention_weights\")\n",
        "    weights = tf.nn.dropout(weights, rate=self.attention_dropout)\n",
        "    old_attention_output = tf.einsum(\"BNFT,BTNH->BFNH\", weights, value)\n",
        "    res.append(old_attention_output)\n",
        "  memory = tf.config.experimental.get_memory_info('GPU:0')\n",
        "  print(\"Memory after attention operation.\")\n",
        "  print(memory['peak'])\n",
        "  print(memory['current'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bw4BCZIopZEF"
      },
      "source": [
        "# Code with manually generated large matrix\n",
        "if (tf.config.list_physical_devices('GPU')):\n",
        "  tf.config.experimental.reset_memory_stats('GPU:0')\n",
        "  memory = tf.config.experimental.get_memory_info('GPU:0')\n",
        "  print(\"Before doing attention operations. Reset the memory\")\n",
        "  print(memory['peak'])\n",
        "  print(memory['current'])\n",
        "  logits = tf.einsum(\"BTNH,BFNH->BNFT\", key_matrix, query_matrix)\n",
        "  #Manually set bias to 0.15\n",
        "  logits += 0.15\n",
        "  weights = tf.nn.softmax(logits, name=\"attention_weights\")\n",
        "  weights = tf.nn.dropout(weights, rate=self.attention_dropout)\n",
        "  old_attention_output = tf.einsum(\"BNFT,BTNH->BFNH\", weights, value_matrix)\n",
        "  memory = tf.config.experimental.get_memory_info('GPU:0')\n",
        "  print(\"Memory after attention operation.\")\n",
        "  print(memory['peak'])\n",
        "  print(memory['current'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAaEXUhyqypc"
      },
      "source": [
        "#@title First fusion function\n",
        "# Fusion Method\n",
        "import numpy as np\n",
        "def fusion(query_input, key, bias, values):\n",
        "    \"\"\"Fuse Logit and Attend operators\n",
        "\n",
        "    Args:\n",
        "      query_input: query output from query_dense_layer with shape [batch_size, length, num_heads, dim_per_head].\n",
        "      key_input: key output from key_dense_layer with shape [batch_size, length, num_heads, dim_per_head].\n",
        "      value_input: value output from value_dense_layer with shape [batch_size, length, num_heads, dim_per_head].\n",
        "      bias_matrix: A tensor with shape [batch_size, 1, length_query, length_source],\n",
        "\n",
        "    Symbol meanings:\n",
        "      B : batch size\n",
        "      T : key_length == length_qeury\n",
        "      N : Head_num\n",
        "      H : dim_per_head\n",
        "      F : query_length == length_source (which also commented as )\n",
        "    Returns:\n",
        "      Attention layer output with shape [batch_size, length_query, hidden_size]\n",
        "\n",
        "      logits = tf.einsum(\"BTNH,BFNH->BNFT\", key, query)\n",
        "      attention_output = tf.einsum(\"BNFT,BTNH->BFNH\", weights, value)\n",
        "\"\"\"\n",
        "  # Randomly set granularity now\n",
        "    batch_granularity = 1\n",
        "    head_granularity = 1\n",
        "    element_granularity = 1\n",
        "\n",
        "    # Get shape of the matrix\n",
        "    batch_size, source_length, head_num, dim = query.shape.as_list()\n",
        "    _,key_length,_,_ = key.shape.as_list()\n",
        "    output = None\n",
        "    assert (head_granularity < head_num)\n",
        "\n",
        "    # The outermost loop loops through batch_size, with granularity as stride\n",
        "    for batch in range(0, batch_size, batch_granularity):\n",
        "      flag = True\n",
        "      suboutput = None\n",
        "      # The inner loop loops through head, with granularity as stride\n",
        "      for head in range(0, head_num, head_granularity):\n",
        "        #Head_Dimension = len(tf.unstack(query, axis = 3))\n",
        "        #for element in range(0, Head_Dimension, element_granularity):\n",
        "        #for loop in range(1):\n",
        "        output_from_unit_batch = np.zeros([batch_granularity, source_length, head_granularity, dim])\n",
        "        # Check each bach inside each granularity\n",
        "        for unit_batch in range(batch, batch + batch_granularity):\n",
        "          output_from_unit_head = np.zeros([source_length, head_granularity, dim])\n",
        "          # Check each head in the head granularity\n",
        "          for unit_head in range(head, head + head_granularity):\n",
        "            #for unit_element in range(element):\n",
        "            logit_output = np.ones([source_length, key_length])\n",
        "            for loop in range(2):\n",
        "              if loop == 0:\n",
        "                #iterate through query_length\n",
        "                for unit_source in range(source_length):\n",
        "                  query_source = query_input[unit_batch, unit_source, unit_head, :]\n",
        "                  query_source = tf.reshape(query_source, [1, query_source.size]) # Tensor with 1 * 10 (unit_source_length * dim_per_head)\n",
        "                  key_source = key[unit_batch, :, unit_head, :] # Tensor with 64 * 10 (key length * dim_per_head)\n",
        "                  #Result is F(1) * T matrix\n",
        "                  #Not sure if we need to do transpose here\n",
        "                  #result = tf.matmul(query_source, key_source) # Tensor with 1 * 64 size (unit_source_length * key_length)\n",
        "                  result = tf.matmul(query_source, tf.transpose(key_source)) # Tensor with 1 * 64 size (unit_source_length * key_length)\n",
        "                  result += bias[unit_batch, :, unit_source, :]\n",
        "                  #Row granularity\n",
        "                  result = tf.nn.softmax(result, name=\"attention_weights\", axis=-1)\n",
        "                  logit_output[unit_source, :] = result\n",
        "                #Logit output now is F * T matrix\n",
        "                logit_output = tf.convert_to_tensor(logit_output)\n",
        "              else:\n",
        "                value_source = values[unit_batch, :, unit_head, :] # Tensor with 64 * 10 (T * H) query_length * dim_per_head\n",
        "                logit_output = tf.cast(logit_output, dtype=float)\n",
        "                value_source = tf.cast(value_source, dtype=float)\n",
        "                attention_output = tf.matmul(logit_output, value_source) #Matrix with size F * H\n",
        "            output_from_unit_head[:, unit_head - head, :] = attention_output\n",
        "          #Size now is F*N*H (output_from_unit_head)\n",
        "          output_from_unit_batch[unit_batch - batch, :, :, :] = output_from_unit_head\n",
        "        # Size now is BFNH (output from unit batch)\n",
        "        if flag == True:\n",
        "          suboutput = output_from_unit_batch\n",
        "          flag = False\n",
        "        else:\n",
        "          suboutput = tf.concat([suboutput, output_from_unit_batch], 2)\n",
        "      if output == None:\n",
        "        output = suboutput\n",
        "      else:\n",
        "        output = tf.concat([output, suboutput], 0)\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtZcKAJ9sImj",
        "outputId": "59eb416b-6ae7-431a-c9aa-f764570a2ebf"
      },
      "source": [
        "for i in range(0, 16, 8):\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfvA9Q2OwbE0",
        "outputId": "5ada4f6f-4593-4cd3-e830-a5f9b37f3761"
      },
      "source": [
        "import tensorflow as tf\n",
        "a = tf.convert_to_tensor(np.array([[1, 0, 1, 0, 1], [0, 1, 0, 1, 0]]))\n",
        "tf.gather(a, [1, 2, 3], axis = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 3), dtype=int64, numpy=\n",
              "array([[0, 1, 0],\n",
              "       [1, 0, 1]])>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4GTFoid0HZt"
      },
      "source": [
        "import numpy as np\n",
        "new_res = []\n",
        "if (tf.config.list_physical_devices('GPU')):\n",
        "  tf.keras.backend.clear_session()\n",
        "  tf.config.experimental.reset_memory_stats('GPU:0')\n",
        "  memory = tf.config.experimental.get_memory_info('GPU:0')\n",
        "  print(\"Before doing attention operations. Reset the memory\")\n",
        "  print(memory['peak'])\n",
        "  print(memory['current'])\n",
        "  batch_size, source_length, head_num, dim = tf.shape(query).numpy()\n",
        "  _,key_length,_,_, = tf.shape(key).numpy()\n",
        "  batch_granularity = 32\n",
        "  head_granularity = 8\n",
        "  # Broadcasting to reshape the bias matrix to the shape as the logit BNFT\n",
        "  bias = tf.reshape(bias, [batch_size, key_length, 1, -1])\n",
        "  # The outermost loop loops through batch_size, with granularity as stride\n",
        "  for batch in tf.range(0, batch_size, batch_granularity):\n",
        "    for head in tf.range(0, head_num, head_granularity):\n",
        "      batch_termination = batch + batch_granularity if batch + batch_granularity <= batch_size else batch_size\n",
        "      for unit_batch in tf.range(batch, batch_termination):\n",
        "        head_termination = head + head_granularity if head + head_granularity <= head_num else head_num\n",
        "        for unit_head in tf.range(head, head + head_granularity):\n",
        "          for loop in tf.range(2):\n",
        "            if loop == 0:\n",
        "                for query_source in tf.split(\n",
        "                    query[unit_batch, :, unit_head, :], num_or_size_splits=1, axis=0):\n",
        "                  key_source = key[unit_batch, :, unit_head, :]\n",
        "                  result = tf.matmul(query_source, tf.transpose(key_source))\n",
        "                  #result = bias[np.random.choice(bias.shape[0])] + result\n",
        "                  #Row granularity\n",
        "                  result = result + 0.15\n",
        "                  result = tf.nn.softmax(result, name=\"attention_weights\")\n",
        "                  logit_output = result\n",
        "            else:\n",
        "              value_source = value[unit_batch, :, unit_head, :] \n",
        "              attention_output = tf.matmul(logit_output, value_source) #Matrix with size F * H\n",
        "          if unit_head == head:\n",
        "            attention_output = tf.expand_dims(attention_output, axis=1)\n",
        "            output_from_unit_head = attention_output\n",
        "          else:\n",
        "           attention_output = tf.expand_dims(attention_output, axis=1)\n",
        "           output_from_unit_head = tf.concat([output_from_unit_head, attention_output], 1)\n",
        "       if unit_batch == batch:\n",
        "         output_from_unit_head = tf.expand_dims(output_from_unit_head, axis=0)\n",
        "         output_from_unit_batch = output_from_unit_head\n",
        "         else:\n",
        "           output_from_unit_head = tf.expand_dims(output_from_unit_head, axis=0)\n",
        "           output_from_unit_batch = tf.concat([output_from_unit_batch, output_from_unit_head], 0)\n",
        "       if head == 0:\n",
        "         suboutput = output_from_unit_batch\n",
        "      else:\n",
        "       suboutput = tf.concat([suboutput, output_from_unit_batch], 2)\n",
        "   if batch == 0:\n",
        "     output = suboutput\n",
        "   else:\n",
        "     output = tf.concat([output, suboutput], 0)\n",
        "   new_res.append(output)\n",
        "  memory = tf.config.experimental.get_memory_info('GPU:0')\n",
        "print(\"Memory profiling using FLAT\")\n",
        "print(memory['peak'])\n",
        "print(memory['current'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KyNUDuzIYSz"
      },
      "source": [
        "import numpy as np\n",
        "new_res = []\n",
        "if (tf.config.list_physical_devices('GPU')):\n",
        "  tf.keras.backend.clear_session()\n",
        "  tf.config.experimental.reset_memory_stats('GPU:0')\n",
        "  memory = tf.config.experimental.get_memory_info('GPU:0')\n",
        "  print(\"Before doing attention operations. Reset the memory\")\n",
        "  print(memory['peak'])\n",
        "  print(memory['current'])\n",
        "\n",
        "  # 将变量整合到function里面，尝试融合多个value矩阵concat形成的大矩阵， 尝试更高的granularity\n",
        "  # logit_output = tf.Variable(0., shape=tf.TensorShape(None))\n",
        "  # key_source = tf.Variable(0., shape=tf.TensorShape(None))\n",
        "  # result = tf.Variable(0., shape=tf.TensorShape(None))\n",
        "  # value_source = tf.Variable(0., shape=tf.TensorShape(None))\n",
        "  # output = tf.Variable(0., shape=tf.TensorShape(None))\n",
        "  # suboutput = tf.Variable(0., shape=tf.TensorShape(None))\n",
        "  # output_from_unit_batch = tf.Variable(0., shape=tf.TensorShape(None))\n",
        "  # output_from_unit_head = tf.Variable(0., shape=tf.TensorShape(None))\n",
        "  for i in range(len(query_matrix)):\n",
        "    query = query_matrix[i]\n",
        "    key = key_matrix[i]\n",
        "    bias = bias_matrix[i]\n",
        "    value = value_matrix[i]\n",
        "    batch_size, source_length, head_num, dim = tf.shape(query).numpy()\n",
        "    _,key_length,_,_, = tf.shape(key).numpy()\n",
        "    batch_granularity = 32\n",
        "    head_granularity = 8\n",
        "    # Broadcasting to reshape the bias matrix to the shape as the logit BNFT\n",
        "    bias = tf.reshape(bias, [batch_size, key_length, 1, -1])\n",
        "    # The outermost loop loops through batch_size, with granularity as stride\n",
        "    for batch in tf.range(0, batch_size, batch_granularity):\n",
        "      for head in tf.range(0, head_num, head_granularity):\n",
        "        batch_termination = batch + batch_granularity if batch + batch_granularity <= batch_size else batch_size\n",
        "        for unit_batch in tf.range(batch, batch_termination):\n",
        "          head_termination = head + head_granularity if head + head_granularity <= head_num else head_num\n",
        "          for unit_head in tf.range(head, head + head_granularity):\n",
        "            for loop in tf.range(2):\n",
        "              if loop == 0:\n",
        "                for unit_source in tf.range(0, source_length):\n",
        "                  # query_source = tf.Variable(0., shape=tf.TensorShape(None))\n",
        "                  query_source = query[unit_batch, unit_source, unit_head, :]\n",
        "                  query_source = tf.reshape(query_source, [1, -1]) # Tensor with 1 * 10 (unit_source_length * dim_per_head)\n",
        "                  key_source = key[unit_batch, :, unit_head, :] # Tensor with 64 * 10 (key length * dim_per_head)\n",
        "                  result = tf.matmul(query_source, tf.transpose(key_source)) # Tensor with 1 * 64 size (unit_source_length * key_length)\n",
        "                  result = bias[unit_batch, unit_source, :, :] + result\n",
        "                  #Row granularity\n",
        "                  result = tf.nn.softmax(result, name=\"attention_weights\")\n",
        "                  if unit_source == 0:\n",
        "                    logit_output = result\n",
        "                  else:\n",
        "                    # Should be shape 45 * 45 here\n",
        "                    logit_output = tf.concat([logit_output, result], axis=0)\n",
        "              else:\n",
        "                # attention_output = tf.Variable(0., shape=tf.TensorShape(None))\n",
        "                value_source = value[unit_batch, :, unit_head, :] # Tensor with 64 * 10 (T * H) query_length * dim_per_head\n",
        "                attention_output = tf.matmul(logit_output, value_source) #Matrix with size F * H\n",
        "            if unit_head == head:\n",
        "              attention_output = tf.expand_dims(attention_output, axis=1)\n",
        "              output_from_unit_head = attention_output\n",
        "            else:\n",
        "              attention_output = tf.expand_dims(attention_output, axis=1)\n",
        "              output_from_unit_head = tf.concat([output_from_unit_head, attention_output], 1)\n",
        "          if unit_batch == batch:\n",
        "            output_from_unit_head = tf.expand_dims(output_from_unit_head, axis=0)\n",
        "            output_from_unit_batch = output_from_unit_head\n",
        "          else:\n",
        "            output_from_unit_head = tf.expand_dims(output_from_unit_head, axis=0)\n",
        "            output_from_unit_batch = tf.concat([output_from_unit_batch, output_from_unit_head], 0)\n",
        "        if head == 0:\n",
        "          suboutput = output_from_unit_batch\n",
        "        else:\n",
        "          suboutput = tf.concat([suboutput, output_from_unit_batch], 2)\n",
        "      if batch == 0:\n",
        "        output = suboutput\n",
        "      else:\n",
        "        output = tf.concat([output, suboutput], 0)\n",
        "    new_res.append(output)\n",
        "    break \n",
        "  memory = tf.config.experimental.get_memory_info('GPU:0')\n",
        "print(\"Memory profiling using FLAT\")\n",
        "print(memory['peak'])\n",
        "print(memory['current'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MeoKqmeIZNi",
        "outputId": "ba450846-586d-4933-9c1e-8c8b35568eca"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "a = tf.convert_to_tensor(np.array([1, 2, 3]))\n",
        "np.random.choice(a, [2, 2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 3],\n",
              "       [3, 2]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reUMf0S7KkXs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
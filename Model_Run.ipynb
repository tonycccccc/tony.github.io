{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73NZiKuYsotY"
      },
      "outputs": [],
      "source": [
        "! pip install tensorflow-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXJYK0hOOwHV"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "saved_model_dir = \"/content/drive/MyDrive/models/transformer/model_big/cp-0001.ckpt\"\n",
        "\n",
        "# Convert the model\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir) # path to the SavedModel directory\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model.\n",
        "with open('model.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ],
      "metadata": {
        "id": "Cqh02J8cpIhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIcbWE56szZB"
      },
      "outputs": [],
      "source": [
        "! nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1uP7gG9sroz"
      },
      "outputs": [],
      "source": [
        "! rm -r models\n",
        "! git clone https://github.com/tensorflow/models.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QBDcLyAsxoU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['PYTHONPATH'] += \":/content/models\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16x9T3UctP8w"
      },
      "outputs": [],
      "source": [
        "! pip3 install --user -r /content/models/official/requirements.txt\n",
        "! pip3 install tensorflow-text-nightly\n",
        "! pip install tf-models-official"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISCFDiNFtSE4"
      },
      "outputs": [],
      "source": [
        "%env PARAM_SET=big\n",
        "%env DATA_DIR=/content/drive/MyDrive/models/transformer/data\n",
        "%env MODEL_DIR=/content/drive/MyDrive/models/transformer/model_big\n",
        "%env VOCAB_FILE=/content/drive/MyDrive/models/transformer/data/vocab.ende.32768"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cPvBkI4u4sK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eacd81b1-20f8-447c-fb3c-3f640b50ca95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models/official/nlp/transformer\n"
          ]
        }
      ],
      "source": [
        "%cd /content/models/official/nlp/transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Yc2IWQjRI2i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea1e3678-08ce-4152-e4a3-d86bdbe86c0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/models/official/nlp/transformer/transformer.py\n"
          ]
        }
      ],
      "source": [
        "#@title transformer.py\n",
        "%%writefile /content/models/official/nlp/transformer/transformer.py\n",
        "# Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"Defines the Transformer model in TF 2.0.\n",
        "\n",
        "Model paper: https://arxiv.org/pdf/1706.03762.pdf\n",
        "Transformer model code source: https://github.com/tensorflow/tensor2tensor\n",
        "\"\"\"\n",
        "from absl import logging\n",
        "\n",
        "import tensorflow as tf\n",
        "from official.nlp.modeling.layers import position_embedding\n",
        "from official.nlp.modeling.ops import beam_search\n",
        "from official.nlp.transformer import attention_layer\n",
        "from official.nlp.transformer import embedding_layer\n",
        "from official.nlp.transformer import ffn_layer\n",
        "from official.nlp.transformer import metrics\n",
        "from official.nlp.transformer import model_utils\n",
        "from official.nlp.transformer.utils.tokenizer import EOS_ID\n",
        "\n",
        "# Disable the not-callable lint error, since it claims many objects are not\n",
        "# callable when they actually are.\n",
        "# pylint: disable=not-callable\n",
        "\n",
        "tf.config.experimental_run_functions_eagerly(True)\n",
        "def create_model(params, is_train):\n",
        "  \"\"\"Creates transformer model.\"\"\"\n",
        "  logging.info(\"Create Transformer Model!!!!\")\n",
        "  with tf.name_scope(\"model\"):\n",
        "    if is_train:\n",
        "      inputs = tf.keras.layers.Input((None,), dtype=\"int64\", name=\"inputs\")\n",
        "      targets = tf.keras.layers.Input((None,), dtype=\"int64\", name=\"targets\")\n",
        "      internal_model = Transformer(params, name=\"transformer_v2\")\n",
        "      logits = internal_model([inputs, targets], training=is_train)\n",
        "      vocab_size = params[\"vocab_size\"]\n",
        "      label_smoothing = params[\"label_smoothing\"]\n",
        "      if params[\"enable_metrics_in_training\"]:\n",
        "        logits = metrics.MetricLayer(vocab_size)([logits, targets])\n",
        "      logits = tf.keras.layers.Lambda(\n",
        "          lambda x: x, name=\"logits\", dtype=tf.float32)(\n",
        "              logits)\n",
        "      model = tf.keras.Model([inputs, targets], logits)\n",
        "      loss = metrics.transformer_loss(logits, targets, label_smoothing,\n",
        "                                      vocab_size)\n",
        "      model.add_loss(loss)\n",
        "      return model\n",
        "\n",
        "    else:\n",
        "      inputs = tf.keras.layers.Input((None,), dtype=\"int64\", name=\"inputs\")\n",
        "      internal_model = Transformer(params, name=\"transformer_v2\")\n",
        "      ret = internal_model([inputs], training=is_train)\n",
        "      outputs, scores = ret[\"outputs\"], ret[\"scores\"]\n",
        "      return tf.keras.Model(inputs, [outputs, scores])\n",
        "\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "  \"\"\"Transformer model with Keras.\n",
        "\n",
        "  Implemented as described in: https://arxiv.org/pdf/1706.03762.pdf\n",
        "\n",
        "  The Transformer model consists of an encoder and decoder. The input is an int\n",
        "  sequence (or a batch of sequences). The encoder produces a continuous\n",
        "  representation, and the decoder uses the encoder output to generate\n",
        "  probabilities for the output sequence.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, params, name=None):\n",
        "    \"\"\"Initialize layers to build Transformer model.\n",
        "\n",
        "    Args:\n",
        "      params: hyperparameter object defining layer sizes, dropout values, etc.\n",
        "      name: name of the model.\n",
        "    \"\"\"\n",
        "    super(Transformer, self).__init__(name=name)\n",
        "    self.params = params\n",
        "    self.embedding_softmax_layer = embedding_layer.EmbeddingSharedWeights(\n",
        "        params[\"vocab_size\"], params[\"hidden_size\"])\n",
        "    self.encoder_stack = EncoderStack(params)\n",
        "    self.decoder_stack = DecoderStack(params)\n",
        "    self.position_embedding = position_embedding.RelativePositionEmbedding(\n",
        "        hidden_size=self.params[\"hidden_size\"])\n",
        "\n",
        "  def get_config(self):\n",
        "    return {\n",
        "        \"params\": self.params,\n",
        "    }\n",
        "\n",
        "  def call(self, inputs, training):\n",
        "    \"\"\"Calculate target logits or inferred target sequences.\n",
        "\n",
        "    Args:\n",
        "      inputs: input tensor list of size 1 or 2.\n",
        "        First item, inputs: int tensor with shape [batch_size, input_length].\n",
        "        Second item (optional), targets: None or int tensor with shape\n",
        "          [batch_size, target_length].\n",
        "      training: boolean, whether in training mode or not.\n",
        "\n",
        "    Returns:\n",
        "      If targets is defined, then return logits for each word in the target\n",
        "      sequence. float tensor with shape [batch_size, target_length, vocab_size]\n",
        "      If target is none, then generate output sequence one token at a time.\n",
        "        returns a dictionary {\n",
        "          outputs: int tensor with shape [batch_size, decoded_length]\n",
        "          scores: float tensor with shape [batch_size]}\n",
        "      Even when float16 is used, the output tensor(s) are always float32.\n",
        "\n",
        "    Raises:\n",
        "      NotImplementedError: If try to use padded decode method on CPU/GPUs.\n",
        "    \"\"\"\n",
        "    logging.info(\"Transformer Task\")\n",
        "    logging.info(tf.executing_eagerly())\n",
        "    inputs = inputs if isinstance(inputs, list) else [inputs]\n",
        "    if len(inputs) == 2:\n",
        "      inputs, targets = inputs[0], inputs[1]\n",
        "    else:\n",
        "      # Decoding path.\n",
        "      inputs, targets = inputs[0], None\n",
        "      if self.params[\"padded_decode\"]:\n",
        "        if not self.params[\"num_replicas\"]:\n",
        "          raise NotImplementedError(\n",
        "              \"Padded decoding on CPU/GPUs is not supported.\")\n",
        "        decode_batch_size = int(self.params[\"decode_batch_size\"] /\n",
        "                                self.params[\"num_replicas\"])\n",
        "        inputs.set_shape([decode_batch_size, self.params[\"decode_max_length\"]])\n",
        "\n",
        "    # Variance scaling is used here because it seems to work in many problems.\n",
        "    # Other reasonable initializers may also work just as well.\n",
        "    with tf.name_scope(\"Transformer\"):\n",
        "      # Calculate attention bias for encoder self-attention and decoder\n",
        "      # multi-headed attention layers.\n",
        "      attention_bias = model_utils.get_padding_bias(inputs)\n",
        "\n",
        "      # Run the inputs through the encoder layer to map the symbol\n",
        "      # representations to continuous representations.\n",
        "      encoder_outputs = self.encode(inputs, attention_bias, training)\n",
        "      # Generate output sequence if targets is None, or return logits if target\n",
        "      # sequence is known.\n",
        "      if targets is None:\n",
        "        return self.predict(encoder_outputs, attention_bias, training)\n",
        "      else:\n",
        "        logits = self.decode(targets, encoder_outputs, attention_bias, training)\n",
        "        return logits\n",
        "\n",
        "  def encode(self, inputs, attention_bias, training):\n",
        "    \"\"\"Generate continuous representation for inputs.\n",
        "\n",
        "    Args:\n",
        "      inputs: int tensor with shape [batch_size, input_length].\n",
        "      attention_bias: float tensor with shape [batch_size, 1, 1, input_length].\n",
        "      training: boolean, whether in training mode or not.\n",
        "\n",
        "    Returns:\n",
        "      float tensor with shape [batch_size, input_length, hidden_size]\n",
        "    \"\"\"\n",
        "    logging.info(\"Start Encoding!!!!!\")\n",
        "    with tf.name_scope(\"encode\"):\n",
        "      # Prepare inputs to the layer stack by adding positional encodings and\n",
        "      # applying dropout.\n",
        "      embedded_inputs = self.embedding_softmax_layer(inputs)\n",
        "      embedded_inputs = tf.cast(embedded_inputs, self.params[\"dtype\"])\n",
        "      inputs_padding = model_utils.get_padding(inputs)\n",
        "      attention_bias = tf.cast(attention_bias, self.params[\"dtype\"])\n",
        "\n",
        "      with tf.name_scope(\"add_pos_encoding\"):\n",
        "        pos_encoding = self.position_embedding(inputs=embedded_inputs)\n",
        "        pos_encoding = tf.cast(pos_encoding, self.params[\"dtype\"])\n",
        "        encoder_inputs = embedded_inputs + pos_encoding\n",
        "\n",
        "      if training:\n",
        "        encoder_inputs = tf.nn.dropout(\n",
        "            encoder_inputs, rate=self.params[\"layer_postprocess_dropout\"])\n",
        "\n",
        "      return self.encoder_stack(\n",
        "          encoder_inputs, attention_bias, inputs_padding, training=training)\n",
        "\n",
        "  def decode(self, targets, encoder_outputs, attention_bias, training):\n",
        "    \"\"\"Generate logits for each value in the target sequence.\n",
        "\n",
        "    Args:\n",
        "      targets: target values for the output sequence. int tensor with shape\n",
        "        [batch_size, target_length]\n",
        "      encoder_outputs: continuous representation of input sequence. float tensor\n",
        "        with shape [batch_size, input_length, hidden_size]\n",
        "      attention_bias: float tensor with shape [batch_size, 1, 1, input_length]\n",
        "      training: boolean, whether in training mode or not.\n",
        "\n",
        "    Returns:\n",
        "      float32 tensor with shape [batch_size, target_length, vocab_size]\n",
        "    \"\"\"\n",
        "    logging.info(\"Start Decoding!!!\")\n",
        "    with tf.name_scope(\"decode\"):\n",
        "      # Prepare inputs to decoder layers by shifting targets, adding positional\n",
        "      # encoding and applying dropout.\n",
        "      decoder_inputs = self.embedding_softmax_layer(targets)\n",
        "      decoder_inputs = tf.cast(decoder_inputs, self.params[\"dtype\"])\n",
        "      attention_bias = tf.cast(attention_bias, self.params[\"dtype\"])\n",
        "      with tf.name_scope(\"shift_targets\"):\n",
        "        # Shift targets to the right, and remove the last element\n",
        "        decoder_inputs = tf.pad(decoder_inputs,\n",
        "                                [[0, 0], [1, 0], [0, 0]])[:, :-1, :]\n",
        "      with tf.name_scope(\"add_pos_encoding\"):\n",
        "        length = tf.shape(decoder_inputs)[1]\n",
        "        pos_encoding = self.position_embedding(decoder_inputs)\n",
        "        pos_encoding = tf.cast(pos_encoding, self.params[\"dtype\"])\n",
        "        decoder_inputs += pos_encoding\n",
        "      if training:\n",
        "        decoder_inputs = tf.nn.dropout(\n",
        "            decoder_inputs, rate=self.params[\"layer_postprocess_dropout\"])\n",
        "\n",
        "      # Run values\n",
        "      decoder_self_attention_bias = model_utils.get_decoder_self_attention_bias(\n",
        "          length, dtype=self.params[\"dtype\"])\n",
        "      outputs = self.decoder_stack(\n",
        "          decoder_inputs,\n",
        "          encoder_outputs,\n",
        "          decoder_self_attention_bias,\n",
        "          attention_bias,\n",
        "          training=training)\n",
        "      logits = self.embedding_softmax_layer(outputs, mode=\"linear\")\n",
        "      logits = tf.cast(logits, tf.float32)\n",
        "      return logits\n",
        "\n",
        "  def _get_symbols_to_logits_fn(self, max_decode_length, training):\n",
        "    \"\"\"Returns a decoding function that calculates logits of the next tokens.\"\"\"\n",
        "    timing_signal = self.position_embedding(\n",
        "        inputs=None, length=max_decode_length + 1)\n",
        "    timing_signal = tf.cast(timing_signal, self.params[\"dtype\"])\n",
        "    decoder_self_attention_bias = model_utils.get_decoder_self_attention_bias(\n",
        "        max_decode_length, dtype=self.params[\"dtype\"])\n",
        "\n",
        "    def symbols_to_logits_fn(ids, i, cache):\n",
        "      \"\"\"Generate logits for next potential IDs.\n",
        "\n",
        "      Args:\n",
        "        ids: Current decoded sequences. int tensor with shape [batch_size *\n",
        "          beam_size, i + 1].\n",
        "        i: Loop index.\n",
        "        cache: dictionary of values storing the encoder output, encoder-decoder\n",
        "          attention bias, and previous decoder attention values.\n",
        "\n",
        "      Returns:\n",
        "        Tuple of\n",
        "          (logits with shape [batch_size * beam_size, vocab_size],\n",
        "           updated cache values)\n",
        "      \"\"\"\n",
        "      # Set decoder input to the last generated IDs\n",
        "      decoder_input = ids[:, -1:]\n",
        "\n",
        "      # Preprocess decoder input by getting embeddings and adding timing signal.\n",
        "      decoder_input = self.embedding_softmax_layer(decoder_input)\n",
        "      decoder_input += timing_signal[i]\n",
        "      if self.params[\"padded_decode\"]:\n",
        "        bias_shape = decoder_self_attention_bias.shape.as_list()\n",
        "        self_attention_bias = tf.slice(\n",
        "            decoder_self_attention_bias, [0, 0, i, 0],\n",
        "            [bias_shape[0], bias_shape[1], 1, bias_shape[3]])\n",
        "      else:\n",
        "        self_attention_bias = decoder_self_attention_bias[:, :, i:i + 1, :i + 1]\n",
        "\n",
        "      decoder_outputs = self.decoder_stack(\n",
        "          decoder_input,\n",
        "          cache.get(\"encoder_outputs\"),\n",
        "          self_attention_bias,\n",
        "          cache.get(\"encoder_decoder_attention_bias\"),\n",
        "          training=training,\n",
        "          cache=cache,\n",
        "          decode_loop_step=i if self.params[\"padded_decode\"] else None)\n",
        "      logits = self.embedding_softmax_layer(decoder_outputs, mode=\"linear\")\n",
        "      logits = tf.squeeze(logits, axis=[1])\n",
        "      return logits, cache\n",
        "\n",
        "    return symbols_to_logits_fn\n",
        "\n",
        "  def predict(self, encoder_outputs, encoder_decoder_attention_bias, training):\n",
        "    \"\"\"Return predicted sequence.\"\"\"\n",
        "    encoder_outputs = tf.cast(encoder_outputs, self.params[\"dtype\"])\n",
        "    if self.params[\"padded_decode\"]:\n",
        "      batch_size = encoder_outputs.shape.as_list()[0]\n",
        "      input_length = encoder_outputs.shape.as_list()[1]\n",
        "    else:\n",
        "      batch_size = tf.shape(encoder_outputs)[0]\n",
        "      input_length = tf.shape(encoder_outputs)[1]\n",
        "    max_decode_length = input_length + self.params[\"extra_decode_length\"]\n",
        "    encoder_decoder_attention_bias = tf.cast(encoder_decoder_attention_bias,\n",
        "                                             self.params[\"dtype\"])\n",
        "\n",
        "    symbols_to_logits_fn = self._get_symbols_to_logits_fn(\n",
        "        max_decode_length, training)\n",
        "\n",
        "    # Create initial set of IDs that will be passed into symbols_to_logits_fn.\n",
        "    initial_ids = tf.zeros([batch_size], dtype=tf.int32)\n",
        "\n",
        "    # Create cache storing decoder attention values for each layer.\n",
        "    # pylint: disable=g-complex-comprehension\n",
        "    init_decode_length = (\n",
        "        max_decode_length if self.params[\"padded_decode\"] else 0)\n",
        "    num_heads = self.params[\"num_heads\"]\n",
        "    dim_per_head = self.params[\"hidden_size\"] // num_heads\n",
        "    cache = {\n",
        "        \"layer_%d\" % layer: {\n",
        "            \"k\":\n",
        "                tf.zeros(\n",
        "                    [batch_size, init_decode_length, num_heads, dim_per_head],\n",
        "                    dtype=self.params[\"dtype\"]),\n",
        "            \"v\":\n",
        "                tf.zeros(\n",
        "                    [batch_size, init_decode_length, num_heads, dim_per_head],\n",
        "                    dtype=self.params[\"dtype\"])\n",
        "        } for layer in range(self.params[\"num_hidden_layers\"])\n",
        "    }\n",
        "    # pylint: enable=g-complex-comprehension\n",
        "\n",
        "    # Add encoder output and attention bias to the cache.\n",
        "    cache[\"encoder_outputs\"] = encoder_outputs\n",
        "    cache[\"encoder_decoder_attention_bias\"] = encoder_decoder_attention_bias\n",
        "\n",
        "    # Use beam search to find the top beam_size sequences and scores.\n",
        "    decoded_ids, scores = beam_search.sequence_beam_search(\n",
        "        symbols_to_logits_fn=symbols_to_logits_fn,\n",
        "        initial_ids=initial_ids,\n",
        "        initial_cache=cache,\n",
        "        vocab_size=self.params[\"vocab_size\"],\n",
        "        beam_size=self.params[\"beam_size\"],\n",
        "        alpha=self.params[\"alpha\"],\n",
        "        max_decode_length=max_decode_length,\n",
        "        eos_id=EOS_ID,\n",
        "        padded_decode=self.params[\"padded_decode\"],\n",
        "        dtype=self.params[\"dtype\"])\n",
        "\n",
        "    # Get the top sequence for each batch element\n",
        "    top_decoded_ids = decoded_ids[:, 0, 1:]\n",
        "    top_scores = scores[:, 0]\n",
        "\n",
        "    return {\"outputs\": top_decoded_ids, \"scores\": top_scores}\n",
        "\n",
        "\n",
        "class PrePostProcessingWrapper(tf.keras.layers.Layer):\n",
        "  \"\"\"Wrapper class that applies layer pre-processing and post-processing.\"\"\"\n",
        "\n",
        "  def __init__(self, layer, params):\n",
        "    super(PrePostProcessingWrapper, self).__init__()\n",
        "    self.layer = layer\n",
        "    self.params = params\n",
        "    self.postprocess_dropout = params[\"layer_postprocess_dropout\"]\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    # Create normalization layer\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization(\n",
        "        epsilon=1e-6, dtype=\"float32\")\n",
        "    super(PrePostProcessingWrapper, self).build(input_shape)\n",
        "\n",
        "  def get_config(self):\n",
        "    return {\n",
        "        \"params\": self.params,\n",
        "    }\n",
        "\n",
        "  def call(self, x, *args, **kwargs):\n",
        "    \"\"\"Calls wrapped layer with same parameters.\"\"\"\n",
        "    # Preprocessing: apply layer normalization\n",
        "    training = kwargs[\"training\"]\n",
        "\n",
        "    y = self.layer_norm(x)\n",
        "\n",
        "    # Get layer output\n",
        "    y = self.layer(y, *args, **kwargs)\n",
        "\n",
        "    # Postprocessing: apply dropout and residual connection\n",
        "    if training:\n",
        "      y = tf.nn.dropout(y, rate=self.postprocess_dropout)\n",
        "    return x + y\n",
        "\n",
        "\n",
        "class EncoderStack(tf.keras.layers.Layer):\n",
        "  \"\"\"Transformer encoder stack.\n",
        "\n",
        "  The encoder stack is made up of N identical layers. Each layer is composed\n",
        "  of the sublayers:\n",
        "    1. Self-attention layer\n",
        "    2. Feedforward network (which is 2 fully-connected layers)\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, params):\n",
        "    super(EncoderStack, self).__init__()\n",
        "    self.params = params\n",
        "    self.layers = []\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    \"\"\"Builds the encoder stack.\"\"\"\n",
        "    params = self.params\n",
        "    for _ in range(1):#params[\"num_hidden_layers\"]):\n",
        "      # Create sublayers for each layer.\n",
        "      logging.info(\"Encoder attention layer added!!!!\")\n",
        "      self_attention_layer = attention_layer.SelfAttention(\n",
        "          params[\"hidden_size\"], params[\"num_heads\"],\n",
        "          params[\"attention_dropout\"])\n",
        "      feed_forward_network = ffn_layer.FeedForwardNetwork(\n",
        "          params[\"hidden_size\"], params[\"filter_size\"], params[\"relu_dropout\"])\n",
        "\n",
        "      self.layers.append([\n",
        "          PrePostProcessingWrapper(self_attention_layer, params),\n",
        "          PrePostProcessingWrapper(feed_forward_network, params)\n",
        "      ])\n",
        "\n",
        "    # Create final layer normalization layer.\n",
        "    self.output_normalization = tf.keras.layers.LayerNormalization(\n",
        "        epsilon=1e-6, dtype=\"float32\")\n",
        "    super(EncoderStack, self).build(input_shape)\n",
        "\n",
        "  def get_config(self):\n",
        "    return {\n",
        "        \"params\": self.params,\n",
        "    }\n",
        "\n",
        "  def call(self, encoder_inputs, attention_bias, inputs_padding, training):\n",
        "    \"\"\"Return the output of the encoder layer stacks.\n",
        "\n",
        "    Args:\n",
        "      encoder_inputs: tensor with shape [batch_size, input_length, hidden_size]\n",
        "      attention_bias: bias for the encoder self-attention layer. [batch_size, 1,\n",
        "        1, input_length]\n",
        "      inputs_padding: tensor with shape [batch_size, input_length], inputs with\n",
        "        zero paddings.\n",
        "      training: boolean, whether in training mode or not.\n",
        "\n",
        "    Returns:\n",
        "      Output of encoder layer stack.\n",
        "      float32 tensor with shape [batch_size, input_length, hidden_size]\n",
        "    \"\"\"\n",
        "    for n, layer in enumerate(self.layers):\n",
        "      # Run inputs through the sublayers.\n",
        "      self_attention_layer = layer[0]\n",
        "      feed_forward_network = layer[1]\n",
        "\n",
        "      with tf.name_scope(\"layer_%d\" % n):\n",
        "        with tf.name_scope(\"self_attention\"):\n",
        "          encoder_inputs = self_attention_layer(\n",
        "              encoder_inputs, attention_bias, training=training)\n",
        "        with tf.name_scope(\"ffn\"):\n",
        "          encoder_inputs = feed_forward_network(\n",
        "              encoder_inputs, training=training)\n",
        "\n",
        "    return self.output_normalization(encoder_inputs)\n",
        "\n",
        "\n",
        "class DecoderStack(tf.keras.layers.Layer):\n",
        "  \"\"\"Transformer decoder stack.\n",
        "\n",
        "  Like the encoder stack, the decoder stack is made up of N identical layers.\n",
        "  Each layer is composed of the sublayers:\n",
        "    1. Self-attention layer\n",
        "    2. Multi-headed attention layer combining encoder outputs with results from\n",
        "       the previous self-attention layer.\n",
        "    3. Feedforward network (2 fully-connected layers)\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, params):\n",
        "    super(DecoderStack, self).__init__()\n",
        "    self.params = params\n",
        "    self.layers = []\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    \"\"\"Builds the decoder stack.\"\"\"\n",
        "    params = self.params\n",
        "    for _ in range(1):#params[\"num_hidden_layers\"]):\n",
        "      logging.info(\"Decoder attention layer added!!!!\")\n",
        "      self_attention_layer = attention_layer.SelfAttention(\n",
        "          params[\"hidden_size\"], params[\"num_heads\"],\n",
        "          params[\"attention_dropout\"])\n",
        "      enc_dec_attention_layer = attention_layer.Attention(\n",
        "          params[\"hidden_size\"], params[\"num_heads\"],\n",
        "          params[\"attention_dropout\"])\n",
        "      feed_forward_network = ffn_layer.FeedForwardNetwork(\n",
        "          params[\"hidden_size\"], params[\"filter_size\"], params[\"relu_dropout\"])\n",
        "\n",
        "      self.layers.append([\n",
        "          PrePostProcessingWrapper(self_attention_layer, params),\n",
        "          PrePostProcessingWrapper(enc_dec_attention_layer, params),\n",
        "          PrePostProcessingWrapper(feed_forward_network, params)\n",
        "      ])\n",
        "    self.output_normalization = tf.keras.layers.LayerNormalization(\n",
        "        epsilon=1e-6, dtype=\"float32\")\n",
        "    super(DecoderStack, self).build(input_shape)\n",
        "\n",
        "  def get_config(self):\n",
        "    return {\n",
        "        \"params\": self.params,\n",
        "    }\n",
        "\n",
        "  def call(self,\n",
        "           decoder_inputs,\n",
        "           encoder_outputs,\n",
        "           decoder_self_attention_bias,\n",
        "           attention_bias,\n",
        "           training,\n",
        "           cache=None,\n",
        "           decode_loop_step=None):\n",
        "    \"\"\"Return the output of the decoder layer stacks.\n",
        "\n",
        "    Args:\n",
        "      decoder_inputs: A tensor with shape [batch_size, target_length,\n",
        "        hidden_size].\n",
        "      encoder_outputs: A tensor with shape [batch_size, input_length,\n",
        "        hidden_size]\n",
        "      decoder_self_attention_bias: A tensor with shape [1, 1, target_len,\n",
        "        target_length], the bias for decoder self-attention layer.\n",
        "      attention_bias: A tensor with shape [batch_size, 1, 1, input_length], the\n",
        "        bias for encoder-decoder attention layer.\n",
        "      training: A bool, whether in training mode or not.\n",
        "      cache: (Used for fast decoding) A nested dictionary storing previous\n",
        "        decoder self-attention values. The items are:\n",
        "          {layer_n: {\"k\": A tensor with shape [batch_size, i, key_channels],\n",
        "                     \"v\": A tensor with shape [batch_size, i, value_channels]},\n",
        "                       ...}\n",
        "      decode_loop_step: An integer, the step number of the decoding loop. Used\n",
        "        only for autoregressive inference on TPU.\n",
        "\n",
        "    Returns:\n",
        "      Output of decoder layer stack.\n",
        "      float32 tensor with shape [batch_size, target_length, hidden_size]\n",
        "    \"\"\"\n",
        "    for n, layer in enumerate(self.layers):\n",
        "      self_attention_layer = layer[0]\n",
        "      enc_dec_attention_layer = layer[1]\n",
        "      feed_forward_network = layer[2]\n",
        "\n",
        "      # Run inputs through the sublayers.\n",
        "      layer_name = \"layer_%d\" % n\n",
        "      layer_cache = cache[layer_name] if cache is not None else None\n",
        "      with tf.name_scope(layer_name):\n",
        "        with tf.name_scope(\"self_attention\"):\n",
        "          decoder_inputs = self_attention_layer(\n",
        "              decoder_inputs,\n",
        "              decoder_self_attention_bias,\n",
        "              training=training,\n",
        "              cache=layer_cache,\n",
        "              decode_loop_step=decode_loop_step)\n",
        "        with tf.name_scope(\"encdec_attention\"):\n",
        "          decoder_inputs = enc_dec_attention_layer(\n",
        "              decoder_inputs,\n",
        "              encoder_outputs,\n",
        "              attention_bias,\n",
        "              training=training)\n",
        "        with tf.name_scope(\"ffn\"):\n",
        "          decoder_inputs = feed_forward_network(\n",
        "              decoder_inputs, training=training)\n",
        "\n",
        "    return self.output_normalization(decoder_inputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24gziLjJoAGy"
      },
      "outputs": [],
      "source": [
        "#Swetha/Harsh: only overwrite transformer.py and transformer_main.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kd5tUvQRuKr",
        "outputId": "08618148-2d61-450d-f132-dd158120d130"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting transformer_main.py\n"
          ]
        }
      ],
      "source": [
        "#@title transformer_main.py\n",
        "%%writefile transformer_main.py\n",
        "# Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"Train and evaluate the Transformer model.\n",
        "\n",
        "See README for description of setting the training schedule and evaluating the\n",
        "BLEU score.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import tempfile\n",
        "\n",
        "# Import libraries\n",
        "from absl import app\n",
        "from absl import flags\n",
        "from absl import logging\n",
        "import tensorflow as tf\n",
        "from official.common import distribute_utils\n",
        "from official.modeling import performance\n",
        "from official.nlp.transformer import compute_bleu\n",
        "from official.nlp.transformer import data_pipeline\n",
        "from official.nlp.transformer import metrics\n",
        "from official.nlp.transformer import misc\n",
        "from official.nlp.transformer import optimizer\n",
        "from official.nlp.transformer import transformer\n",
        "from official.nlp.transformer import translate\n",
        "from official.nlp.transformer.utils import tokenizer\n",
        "from official.utils.flags import core as flags_core\n",
        "from official.utils.misc import keras_utils\n",
        "# pylint:disable=logging-format-interpolation\n",
        "\n",
        "INF = int(1e9)\n",
        "BLEU_DIR = \"bleu\"\n",
        "_SINGLE_SAMPLE = 1\n",
        "tf.config.experimental_run_functions_eagerly(True)\n",
        "\n",
        "def translate_and_compute_bleu(model,\n",
        "                               params,\n",
        "                               subtokenizer,\n",
        "                               bleu_source,\n",
        "                               bleu_ref,\n",
        "                               distribution_strategy=None):\n",
        "  \"\"\"Translate file and report the cased and uncased bleu scores.\n",
        "\n",
        "  Args:\n",
        "    model: A Keras model, used to generate the translations.\n",
        "    params: A dictionary, containing the translation related parameters.\n",
        "    subtokenizer: A subtokenizer object, used for encoding and decoding source\n",
        "      and translated lines.\n",
        "    bleu_source: A file containing source sentences for translation.\n",
        "    bleu_ref: A file containing the reference for the translated sentences.\n",
        "    distribution_strategy: A platform distribution strategy, used for TPU based\n",
        "      translation.\n",
        "\n",
        "  Returns:\n",
        "    uncased_score: A float, the case insensitive BLEU score.\n",
        "    cased_score: A float, the case sensitive BLEU score.\n",
        "  \"\"\"\n",
        "  # Create temporary file to store translation.\n",
        "  tmp = tempfile.NamedTemporaryFile(delete=False)\n",
        "  tmp_filename = tmp.name\n",
        "\n",
        "  translate.translate_file(\n",
        "      model,\n",
        "      params,\n",
        "      subtokenizer,\n",
        "      bleu_source,\n",
        "      output_file=tmp_filename,\n",
        "      print_all_translations=False,\n",
        "      distribution_strategy=distribution_strategy)\n",
        "\n",
        "  # Compute uncased and cased bleu scores.\n",
        "  uncased_score = compute_bleu.bleu_wrapper(bleu_ref, tmp_filename, False)\n",
        "  cased_score = compute_bleu.bleu_wrapper(bleu_ref, tmp_filename, True)\n",
        "  os.remove(tmp_filename)\n",
        "  return uncased_score, cased_score\n",
        "\n",
        "\n",
        "def evaluate_and_log_bleu(model,\n",
        "                          params,\n",
        "                          bleu_source,\n",
        "                          bleu_ref,\n",
        "                          vocab_file,\n",
        "                          distribution_strategy=None):\n",
        "  \"\"\"Calculate and record the BLEU score.\n",
        "\n",
        "  Args:\n",
        "    model: A Keras model, used to generate the translations.\n",
        "    params: A dictionary, containing the translation related parameters.\n",
        "    bleu_source: A file containing source sentences for translation.\n",
        "    bleu_ref: A file containing the reference for the translated sentences.\n",
        "    vocab_file: A file containing the vocabulary for translation.\n",
        "    distribution_strategy: A platform distribution strategy, used for TPU based\n",
        "      translation.\n",
        "\n",
        "  Returns:\n",
        "    uncased_score: A float, the case insensitive BLEU score.\n",
        "    cased_score: A float, the case sensitive BLEU score.\n",
        "  \"\"\"\n",
        "  subtokenizer = tokenizer.Subtokenizer(vocab_file)\n",
        "\n",
        "  uncased_score, cased_score = translate_and_compute_bleu(\n",
        "      model, params, subtokenizer, bleu_source, bleu_ref, distribution_strategy)\n",
        "\n",
        "  logging.info(\"Bleu score (uncased): %s\", uncased_score)\n",
        "  logging.info(\"Bleu score (cased): %s\", cased_score)\n",
        "  return uncased_score, cased_score\n",
        "\n",
        "\n",
        "class TransformerTask(object):\n",
        "  \"\"\"Main entry of Transformer model.\"\"\"\n",
        "\n",
        "  def __init__(self, flags_obj):\n",
        "    \"\"\"Init function of TransformerMain.\n",
        "\n",
        "    Args:\n",
        "      flags_obj: Object containing parsed flag values, i.e., FLAGS.\n",
        "\n",
        "    Raises:\n",
        "      ValueError: if not using static batch for input data on TPU.\n",
        "    \"\"\"\n",
        "    self.flags_obj = flags_obj\n",
        "    self.predict_model = None\n",
        "\n",
        "    # Add flag-defined parameters to params object\n",
        "    num_gpus = flags_core.get_num_gpus(flags_obj)\n",
        "    self.params = params = misc.get_model_params(flags_obj.param_set, num_gpus)\n",
        "\n",
        "    params[\"num_gpus\"] = num_gpus\n",
        "    params[\"use_ctl\"] = flags_obj.use_ctl\n",
        "    params[\"data_dir\"] = flags_obj.data_dir\n",
        "    params[\"model_dir\"] = flags_obj.model_dir\n",
        "    params[\"static_batch\"] = flags_obj.static_batch\n",
        "    params[\"max_length\"] = flags_obj.max_length\n",
        "    params[\"decode_batch_size\"] = flags_obj.decode_batch_size\n",
        "    params[\"decode_max_length\"] = flags_obj.decode_max_length\n",
        "    params[\"padded_decode\"] = flags_obj.padded_decode\n",
        "    params[\"max_io_parallelism\"] = (\n",
        "        flags_obj.num_parallel_calls or tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    params[\"use_synthetic_data\"] = flags_obj.use_synthetic_data\n",
        "    params[\"batch_size\"] = flags_obj.batch_size or params[\"default_batch_size\"]\n",
        "    params[\"repeat_dataset\"] = None\n",
        "    params[\"dtype\"] = flags_core.get_tf_dtype(flags_obj)\n",
        "    params[\"enable_tensorboard\"] = flags_obj.enable_tensorboard\n",
        "    params[\"enable_metrics_in_training\"] = flags_obj.enable_metrics_in_training\n",
        "    params[\"steps_between_evals\"] = flags_obj.steps_between_evals\n",
        "    params[\"enable_checkpointing\"] = flags_obj.enable_checkpointing\n",
        "    params[\"save_weights_only\"] = flags_obj.save_weights_only\n",
        "    #Swetha: debug\n",
        "    if not flags_obj.save_weights_only:\n",
        "      logging.info(\"Swetha debug:save weights only is false\")\n",
        "\n",
        "    self.distribution_strategy = distribute_utils.get_distribution_strategy(\n",
        "        distribution_strategy=flags_obj.distribution_strategy,\n",
        "        num_gpus=num_gpus,\n",
        "        all_reduce_alg=flags_obj.all_reduce_alg,\n",
        "        num_packs=flags_obj.num_packs,\n",
        "        tpu_address=flags_obj.tpu or \"\")\n",
        "    if self.use_tpu:\n",
        "      params[\"num_replicas\"] = self.distribution_strategy.num_replicas_in_sync\n",
        "    else:\n",
        "      logging.info(\"Running transformer with num_gpus = %d\", num_gpus)\n",
        "\n",
        "    if self.distribution_strategy:\n",
        "      logging.info(\"For training, using distribution strategy: %s\",\n",
        "                   self.distribution_strategy)\n",
        "    else:\n",
        "      logging.info(\"Not using any distribution strategy.\")\n",
        "\n",
        "    performance.set_mixed_precision_policy(params[\"dtype\"])\n",
        "\n",
        "  @property\n",
        "  def use_tpu(self):\n",
        "    if self.distribution_strategy:\n",
        "      return isinstance(self.distribution_strategy, tf.distribute.TPUStrategy)\n",
        "    return False\n",
        "\n",
        "  def train(self):\n",
        "    \"\"\"Trains the model.\"\"\"\n",
        "    params = self.params\n",
        "    flags_obj = self.flags_obj\n",
        "    # Sets config options.\n",
        "    keras_utils.set_session_config(enable_xla=flags_obj.enable_xla)\n",
        "\n",
        "    _ensure_dir(flags_obj.model_dir)\n",
        "    with distribute_utils.get_strategy_scope(self.distribution_strategy):\n",
        "      model = transformer.create_model(params, is_train=True)\n",
        "      opt = self._create_optimizer()\n",
        "\n",
        "      current_step = 0\n",
        "      checkpoint = tf.train.Checkpoint(model=model, optimizer=opt)\n",
        "      latest_checkpoint = tf.train.latest_checkpoint(flags_obj.model_dir)\n",
        "      # if latest_checkpoint:\n",
        "      #   checkpoint.restore(latest_checkpoint)\n",
        "      #   logging.info(\"Loaded checkpoint %s\", latest_checkpoint)\n",
        "      #   current_step = opt.iterations.numpy()\n",
        "\n",
        "      if params[\"use_ctl\"]:\n",
        "        train_loss_metric = tf.keras.metrics.Mean(\n",
        "            \"training_loss\", dtype=tf.float32)\n",
        "        if params[\"enable_tensorboard\"]:\n",
        "          summary_writer = tf.summary.create_file_writer(\n",
        "              os.path.join(flags_obj.model_dir, \"summary\"))\n",
        "        else:\n",
        "          summary_writer = tf.summary.create_noop_writer()\n",
        "        train_metrics = [train_loss_metric]\n",
        "        if params[\"enable_metrics_in_training\"]:\n",
        "          train_metrics = train_metrics + model.metrics\n",
        "      else:\n",
        "        #model.compile(opt)\n",
        "        model.compile(opt, run_eagerly=True)\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    if self.use_tpu:\n",
        "      # Different from experimental_distribute_dataset,\n",
        "      # distribute_datasets_from_function requires\n",
        "      # per-replica/local batch size.\n",
        "      params[\"batch_size\"] /= self.distribution_strategy.num_replicas_in_sync\n",
        "      train_ds = (\n",
        "          self.distribution_strategy.distribute_datasets_from_function(\n",
        "              lambda ctx: data_pipeline.train_input_fn(params, ctx)))\n",
        "    else:\n",
        "      train_ds = data_pipeline.train_input_fn(params)\n",
        "      map_data_fn = data_pipeline.map_data_for_transformer_fn\n",
        "      train_ds = train_ds.map(\n",
        "          map_data_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    if params[\"use_ctl\"]:\n",
        "      train_ds_iterator = iter(train_ds)\n",
        "\n",
        "    callbacks = self._create_callbacks(flags_obj.model_dir, params)\n",
        "\n",
        "    # Only TimeHistory callback is supported for CTL\n",
        "    if params[\"use_ctl\"]:\n",
        "      callbacks = [cb for cb in callbacks\n",
        "                   if isinstance(cb, keras_utils.TimeHistory)]\n",
        "\n",
        "    #@tf.function\n",
        "    def train_steps(iterator, steps):\n",
        "      \"\"\"Training steps function for TPU runs.\n",
        "\n",
        "      Args:\n",
        "        iterator: The input iterator of the training dataset.\n",
        "        steps: An integer, the number of training steps.\n",
        "\n",
        "      Returns:\n",
        "        A float, the loss value.\n",
        "      \"\"\"\n",
        "\n",
        "      def _step_fn(inputs):\n",
        "        \"\"\"Per-replica step function.\"\"\"\n",
        "        inputs, targets = inputs\n",
        "        logits = model([inputs, targets], training=True)\n",
        "        loss = metrics.transformer_loss(logits, targets,\n",
        "                                          params[\"label_smoothing\"],\n",
        "                                          params[\"vocab_size\"])\n",
        "        # with tf.GradientTape() as tape:\n",
        "        #   logits = model([inputs, targets], training=True)\n",
        "        #   # Scales the loss, which results in using the average loss across all\n",
        "        #   # of the replicas for backprop.\n",
        "        #   scaled_loss = loss / self.distribution_strategy.num_replicas_in_sync\n",
        "\n",
        "        # # De-dupes variables due to keras tracking issues.\n",
        "        # tvars = list({id(v): v for v in model.trainable_variables}.values())\n",
        "        # grads = tape.gradient(scaled_loss, tvars)\n",
        "        # opt.apply_gradients(zip(grads, tvars))\n",
        "        # # For reporting, the metric takes the mean of losses.\n",
        "        train_loss_metric.update_state(loss)\n",
        "\n",
        "      for _ in tf.range(steps):\n",
        "        train_loss_metric.reset_states()\n",
        "        self.distribution_strategy.run(\n",
        "            _step_fn, args=(next(iterator),))\n",
        "\n",
        "    cased_score, uncased_score = None, None\n",
        "    cased_score_history, uncased_score_history = [], []\n",
        "    while current_step < flags_obj.train_steps:\n",
        "      remaining_steps = flags_obj.train_steps - current_step\n",
        "      train_steps_per_eval = (\n",
        "          remaining_steps if remaining_steps < flags_obj.steps_between_evals\n",
        "          else flags_obj.steps_between_evals)\n",
        "      current_iteration = current_step // flags_obj.steps_between_evals\n",
        "\n",
        "      logging.info(\n",
        "          \"Start train iteration at global step:{}\".format(current_step))\n",
        "      history = None\n",
        "      if params[\"use_ctl\"]:\n",
        "        if not self.use_tpu:\n",
        "          raise NotImplementedError(\n",
        "              \"Custom training loop on GPUs is not implemented.\")\n",
        "\n",
        "        # Runs training steps.\n",
        "        with summary_writer.as_default():\n",
        "          for cb in callbacks:\n",
        "            cb.on_epoch_begin(current_iteration)\n",
        "            cb.on_batch_begin(0)\n",
        "\n",
        "          train_steps(\n",
        "              train_ds_iterator,\n",
        "              tf.convert_to_tensor(train_steps_per_eval, dtype=tf.int32))\n",
        "          current_step += train_steps_per_eval\n",
        "          train_loss = train_loss_metric.result().numpy().astype(float)\n",
        "          logging.info(\"Train Step: %d/%d / loss = %s\", current_step,\n",
        "                       flags_obj.train_steps, train_loss)\n",
        "\n",
        "          for cb in callbacks:\n",
        "            cb.on_batch_end(train_steps_per_eval - 1)\n",
        "            cb.on_epoch_end(current_iteration)\n",
        "\n",
        "          if params[\"enable_tensorboard\"]:\n",
        "            for metric_obj in train_metrics:\n",
        "              tf.summary.scalar(metric_obj.name, metric_obj.result(),\n",
        "                                current_step)\n",
        "              summary_writer.flush()\n",
        "\n",
        "        for cb in callbacks:\n",
        "          cb.on_train_end()\n",
        "\n",
        "        #Swetha: debug\n",
        "        if not flags_obj.enable_checkpointing:\n",
        "          logging.info(\"Swetha: no checkpointing enabled\")\n",
        "\n",
        "        if flags_obj.enable_checkpointing:\n",
        "          #Swetha: debug\n",
        "          logging.info(\"Checkpointing is enabled\\n\")\n",
        "          # avoid check-pointing when running for benchmarking.\n",
        "          checkpoint_name = checkpoint.save(\n",
        "              os.path.join(flags_obj.model_dir,\n",
        "                           \"ctl_step_{}.ckpt\".format(current_step)))\n",
        "          logging.info(\"Saved checkpoint to %s\", checkpoint_name)\n",
        "      else:\n",
        "        if self.use_tpu:\n",
        "          raise NotImplementedError(\n",
        "              \"Keras model.fit on TPUs is not implemented.\")\n",
        "        history = model.fit(\n",
        "            train_ds,\n",
        "            initial_epoch=current_iteration,\n",
        "            epochs=current_iteration + 1,\n",
        "            steps_per_epoch=train_steps_per_eval,\n",
        "            callbacks=callbacks,\n",
        "            # If TimeHistory is enabled, progress bar would be messy. Increase\n",
        "            # the verbose level to get rid of it.\n",
        "            verbose=(2 if flags_obj.enable_time_history else 1))\n",
        "        current_step += train_steps_per_eval\n",
        "        logging.info(\"Train history: {}\".format(history.history))\n",
        "\n",
        "      #Swetha:debug\n",
        "      logging.info(\"All done\\n\")\n",
        "      if flags_obj.enable_checkpointing:\n",
        "          #Swetha: debug\n",
        "          logging.info(\"Checkpointing is enabled\\n\")\n",
        "      logging.info(\"End train iteration at global step:{}\".format(current_step))\n",
        "\n",
        "      # if (flags_obj.bleu_source and flags_obj.bleu_ref):\n",
        "      #   uncased_score, cased_score = self.eval()\n",
        "      #   cased_score_history.append([current_iteration + 1, cased_score])\n",
        "      #   uncased_score_history.append([current_iteration + 1, uncased_score])\n",
        "\n",
        "    stats = ({\n",
        "        \"loss\": train_loss\n",
        "    } if history is None else {})\n",
        "    misc.update_stats(history, stats, callbacks)\n",
        "    if uncased_score and cased_score:\n",
        "      stats[\"bleu_uncased\"] = uncased_score\n",
        "      stats[\"bleu_cased\"] = cased_score\n",
        "      stats[\"bleu_uncased_history\"] = uncased_score_history\n",
        "      stats[\"bleu_cased_history\"] = cased_score_history\n",
        "    return stats\n",
        "\n",
        "  def eval(self):\n",
        "    \"\"\"Evaluates the model.\"\"\"\n",
        "    distribution_strategy = self.distribution_strategy if self.use_tpu else None\n",
        "\n",
        "    # We only want to create the model under DS scope for TPU case.\n",
        "    # When 'distribution_strategy' is None, a no-op DummyContextManager will\n",
        "    # be used.\n",
        "    with distribute_utils.get_strategy_scope(distribution_strategy):\n",
        "      if not self.predict_model:\n",
        "        self.predict_model = transformer.create_model(self.params, False)\n",
        "      self._load_weights_if_possible(\n",
        "          self.predict_model,\n",
        "          tf.train.latest_checkpoint(self.flags_obj.model_dir))\n",
        "      self.predict_model.summary()\n",
        "    return evaluate_and_log_bleu(\n",
        "        self.predict_model, self.params, self.flags_obj.bleu_source,\n",
        "        self.flags_obj.bleu_ref, self.flags_obj.vocab_file,\n",
        "        distribution_strategy)\n",
        "\n",
        "  def predict(self):\n",
        "    \"\"\"Predicts result from the model.\"\"\"\n",
        "    params = self.params\n",
        "    flags_obj = self.flags_obj\n",
        "\n",
        "    with tf.name_scope(\"model\"):\n",
        "      model = transformer.create_model(params, is_train=False)\n",
        "      self._load_weights_if_possible(\n",
        "          model, tf.train.latest_checkpoint(self.flags_obj.model_dir))\n",
        "      model.summary()\n",
        "    subtokenizer = tokenizer.Subtokenizer(flags_obj.vocab_file)\n",
        "\n",
        "    ds = data_pipeline.eval_input_fn(params)\n",
        "    ds = ds.map(lambda x, y: x).take(_SINGLE_SAMPLE)\n",
        "    ret = model.predict(ds)\n",
        "    val_outputs, _ = ret\n",
        "    length = len(val_outputs)\n",
        "    for i in range(length):\n",
        "      translate.translate_from_input(val_outputs[i], subtokenizer)\n",
        "\n",
        "  def _create_callbacks(self, cur_log_dir, params):\n",
        "    \"\"\"Creates a list of callbacks.\"\"\"\n",
        "    callbacks = misc.get_callbacks()\n",
        "    #Swetha: debug\n",
        "    if params[\"save_weights_only\"] == False:\n",
        "      logging.info(\"save_weights_only is False.\\n\")\n",
        "    if params[\"enable_checkpointing\"]:\n",
        "      ckpt_full_path = os.path.join(cur_log_dir, \"cp-{epoch:04d}.ckpt\")\n",
        "      callbacks.append(\n",
        "          tf.keras.callbacks.ModelCheckpoint(\n",
        "              ckpt_full_path, save_weights_only=params[\"save_weights_only\"]))\n",
        "    return callbacks\n",
        "\n",
        "  def _load_weights_if_possible(self, model, init_weight_path=None):\n",
        "    \"\"\"Loads model weights when it is provided.\"\"\"\n",
        "    if init_weight_path:\n",
        "      logging.info(\"Load weights: {}\".format(init_weight_path))\n",
        "      if self.use_tpu:\n",
        "        checkpoint = tf.train.Checkpoint(\n",
        "            model=model, optimizer=self._create_optimizer())\n",
        "        checkpoint.restore(init_weight_path)\n",
        "      else:\n",
        "        model.load_weights(init_weight_path)\n",
        "    else:\n",
        "      logging.info(\"Weights not loaded from path:{}\".format(init_weight_path))\n",
        "\n",
        "  def _create_optimizer(self):\n",
        "    \"\"\"Creates optimizer.\"\"\"\n",
        "    params = self.params\n",
        "    lr_schedule = optimizer.LearningRateSchedule(\n",
        "        params[\"learning_rate\"], params[\"hidden_size\"],\n",
        "        params[\"learning_rate_warmup_steps\"])\n",
        "    opt = tf.keras.optimizers.Adam(\n",
        "        lr_schedule,\n",
        "        params[\"optimizer_adam_beta1\"],\n",
        "        params[\"optimizer_adam_beta2\"],\n",
        "        epsilon=params[\"optimizer_adam_epsilon\"])\n",
        "\n",
        "    opt = performance.configure_optimizer(\n",
        "        opt,\n",
        "        use_float16=params[\"dtype\"] == tf.float16,\n",
        "        use_graph_rewrite=self.flags_obj.fp16_implementation == \"graph_rewrite\",\n",
        "        loss_scale=flags_core.get_loss_scale(\n",
        "            self.flags_obj, default_for_fp16=\"dynamic\"))\n",
        "\n",
        "    return opt\n",
        "\n",
        "\n",
        "def _ensure_dir(log_dir):\n",
        "  \"\"\"Makes log dir if not existed.\"\"\"\n",
        "  if not tf.io.gfile.exists(log_dir):\n",
        "    tf.io.gfile.makedirs(log_dir)\n",
        "\n",
        "\n",
        "def main(_):\n",
        "  flags_obj = flags.FLAGS\n",
        "  if flags_obj.enable_mlir_bridge:\n",
        "    tf.config.experimental.enable_mlir_bridge()\n",
        "  task = TransformerTask(flags_obj)\n",
        "\n",
        "  # Execute flag override logic for better model performance\n",
        "  if flags_obj.tf_gpu_thread_mode:\n",
        "    keras_utils.set_gpu_thread_mode_and_count(\n",
        "        per_gpu_thread_count=flags_obj.per_gpu_thread_count,\n",
        "        gpu_thread_mode=flags_obj.tf_gpu_thread_mode,\n",
        "        num_gpus=flags_obj.num_gpus,\n",
        "        datasets_num_private_threads=flags_obj.datasets_num_private_threads)\n",
        "\n",
        "  if flags_obj.mode == \"train\":\n",
        "    task.train()\n",
        "  elif flags_obj.mode == \"predict\":\n",
        "    task.predict()\n",
        "  elif flags_obj.mode == \"eval\":\n",
        "    task.eval()\n",
        "  else:\n",
        "    raise ValueError(\"Invalid mode {}\".format(flags_obj.mode))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  logging.set_verbosity(logging.INFO)\n",
        "  tf.config.run_functions_eagerly(True)\n",
        "  misc.define_transformer_flags()\n",
        "  app.run(main)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIrmhVFeSeYv",
        "outputId": "20787bf4-3d2b-436e-e12e-5e9e304059bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting misc.py\n"
          ]
        }
      ],
      "source": [
        "#@title Configuration File Rewrite\n",
        "#Overwrite the configuration file misc.py\n",
        "%%writefile misc.py\n",
        "# Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"Misc for Transformer.\"\"\"\n",
        "\n",
        "# pylint: disable=g-bad-import-order\n",
        "\n",
        "from absl import flags\n",
        "import tensorflow as tf\n",
        "\n",
        "from official.nlp.transformer import model_params\n",
        "from official.utils.flags import core as flags_core\n",
        "from official.utils.misc import keras_utils\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "PARAMS_MAP = {\n",
        "    'tiny': model_params.TINY_PARAMS,\n",
        "    'base': model_params.BASE_PARAMS,\n",
        "    'big': model_params.BIG_PARAMS,\n",
        "}\n",
        "\n",
        "\n",
        "def get_model_params(param_set, num_gpus):\n",
        "  \"\"\"Gets predefined model params.\"\"\"\n",
        "  if num_gpus > 1:\n",
        "    if param_set == 'big':\n",
        "      return model_params.BIG_MULTI_GPU_PARAMS.copy()\n",
        "    elif param_set == 'base':\n",
        "      return model_params.BASE_MULTI_GPU_PARAMS.copy()\n",
        "    else:\n",
        "      raise ValueError('Not valid params: param_set={} num_gpus={}'.format(\n",
        "          param_set, num_gpus))\n",
        "\n",
        "  return PARAMS_MAP[param_set].copy()\n",
        "\n",
        "\n",
        "def define_transformer_flags():\n",
        "  \"\"\"Add flags and flag validators for running transformer_main.\"\"\"\n",
        "  # Add common flags (data_dir, model_dir, etc.).\n",
        "  flags_core.define_base(num_gpu=True, distribution_strategy=True)\n",
        "  flags_core.define_performance(\n",
        "      num_parallel_calls=True,\n",
        "      inter_op=False,\n",
        "      intra_op=False,\n",
        "      synthetic_data=True,\n",
        "      max_train_steps=False,\n",
        "      dtype=True,\n",
        "      loss_scale=True,\n",
        "      all_reduce_alg=True,\n",
        "      num_packs=True,\n",
        "      tf_gpu_thread_mode=True,\n",
        "      datasets_num_private_threads=True,\n",
        "      enable_xla=True,\n",
        "      fp16_implementation=True)\n",
        "\n",
        "  flags_core.define_benchmark()\n",
        "  flags_core.define_device(tpu=True)\n",
        "\n",
        "  flags.DEFINE_integer(\n",
        "      name='train_steps',\n",
        "      short_name='ts',\n",
        "      default=300000,\n",
        "      help=flags_core.help_wrap('The number of steps used to train.'))\n",
        "  flags.DEFINE_integer(\n",
        "      name='steps_between_evals',\n",
        "      short_name='sbe',\n",
        "      default=5000,\n",
        "      help=flags_core.help_wrap(\n",
        "          'The Number of training steps to run between evaluations. This is '\n",
        "          'used if --train_steps is defined.'))\n",
        "  flags.DEFINE_boolean(\n",
        "      name='enable_time_history',\n",
        "      default=True,\n",
        "      help='Whether to enable TimeHistory callback.')\n",
        "  flags.DEFINE_boolean(\n",
        "      name='enable_tensorboard',\n",
        "      default=False,\n",
        "      help='Whether to enable Tensorboard callback.')\n",
        "  flags.DEFINE_boolean(\n",
        "      name='enable_metrics_in_training',\n",
        "      default=False,\n",
        "      help='Whether to enable metrics during training.')\n",
        "  flags.DEFINE_boolean(\n",
        "      name='enable_mlir_bridge',\n",
        "      default=False,\n",
        "      help='Whether to enable the TF to XLA bridge.')\n",
        "  # Set flags from the flags_core module as 'key flags' so they're listed when\n",
        "  # the '-h' flag is used. Without this line, the flags defined above are\n",
        "  # only shown in the full `--helpful` help text.\n",
        "  flags.adopt_module_key_flags(flags_core)\n",
        "\n",
        "  # Add transformer-specific flags\n",
        "  flags.DEFINE_enum(\n",
        "      name='param_set',\n",
        "      short_name='mp',\n",
        "      default='big',\n",
        "      enum_values=PARAMS_MAP.keys(),\n",
        "      help=flags_core.help_wrap(\n",
        "          'Parameter set to use when creating and training the model. The '\n",
        "          'parameters define the input shape (batch size and max length), '\n",
        "          'model configuration (size of embedding, # of hidden layers, etc.), '\n",
        "          'and various other settings. The big parameter set increases the '\n",
        "          'default batch size, embedding/hidden size, and filter size. For a '\n",
        "          'complete list of parameters, please see model/model_params.py.'))\n",
        "\n",
        "  flags.DEFINE_bool(\n",
        "      name='static_batch',\n",
        "      short_name='sb',\n",
        "      default=True,\n",
        "      help=flags_core.help_wrap(\n",
        "          'Whether the batches in the dataset should have static shapes. In '\n",
        "          'general, this setting should be False. Dynamic shapes allow the '\n",
        "          'inputs to be grouped so that the number of padding tokens is '\n",
        "          'minimized, and helps model training. In cases where the input shape '\n",
        "          'must be static (e.g. running on TPU), this setting will be ignored '\n",
        "          'and static batching will always be used.'))\n",
        "  flags.DEFINE_integer(\n",
        "      name='max_length',\n",
        "      short_name='ml',\n",
        "      default=256,\n",
        "      help=flags_core.help_wrap(\n",
        "          'Max sentence length for Transformer. Default is 256. Note: Usually '\n",
        "          'it is more effective to use a smaller max length if static_batch is '\n",
        "          'enabled, e.g. 64.'))\n",
        "\n",
        "  # Flags for training with steps (may be used for debugging)\n",
        "  flags.DEFINE_integer(\n",
        "      name='validation_steps',\n",
        "      short_name='vs',\n",
        "      default=64,\n",
        "      help=flags_core.help_wrap('The number of steps used in validation.'))\n",
        "\n",
        "  # BLEU score computation\n",
        "  flags.DEFINE_string(\n",
        "      name='bleu_source',\n",
        "      short_name='bls',\n",
        "      default=None,\n",
        "      help=flags_core.help_wrap(\n",
        "          'Path to source file containing text translate when calculating the '\n",
        "          'official BLEU score. Both --bleu_source and --bleu_ref must be set. '\n",
        "      ))\n",
        "  flags.DEFINE_string(\n",
        "      name='bleu_ref',\n",
        "      short_name='blr',\n",
        "      default=None,\n",
        "      help=flags_core.help_wrap(\n",
        "          'Path to source file containing text translate when calculating the '\n",
        "          'official BLEU score. Both --bleu_source and --bleu_ref must be set. '\n",
        "      ))\n",
        "  flags.DEFINE_string(\n",
        "      name='vocab_file',\n",
        "      short_name='vf',\n",
        "      default=None,\n",
        "      help=flags_core.help_wrap(\n",
        "          'Path to subtoken vocabulary file. If data_download.py was used to '\n",
        "          'download and encode the training data, look in the data_dir to find '\n",
        "          'the vocab file.'))\n",
        "  flags.DEFINE_string(\n",
        "      name='mode',\n",
        "      default='train',\n",
        "      help=flags_core.help_wrap('mode: train, eval, or predict'))\n",
        "  flags.DEFINE_bool(\n",
        "      name='use_ctl',\n",
        "      default=False,\n",
        "      help=flags_core.help_wrap(\n",
        "          'Whether the model runs with custom training loop.'))\n",
        "  flags.DEFINE_integer(\n",
        "      name='decode_batch_size',\n",
        "      default=256,\n",
        "      help=flags_core.help_wrap(\n",
        "          'Global batch size used for Transformer autoregressive decoding on '\n",
        "          'TPU.'))\n",
        "  flags.DEFINE_integer(\n",
        "      name='decode_max_length',\n",
        "      default=256,\n",
        "      help=flags_core.help_wrap(\n",
        "          'Max sequence length of the decode/eval data. This is used by '\n",
        "          'Transformer autoregressive decoding on TPU to have minimum '\n",
        "          'paddings.'))\n",
        "  flags.DEFINE_bool(\n",
        "      name='padded_decode',\n",
        "      default=True,\n",
        "      help=flags_core.help_wrap(\n",
        "          'Whether the autoregressive decoding runs with input data padded to '\n",
        "          'the decode_max_length. For TPU/XLA-GPU runs, this flag has to be '\n",
        "          'set due the static shape requirement. Although CPU/GPU could also '\n",
        "          'use padded_decode, it has not been tested. In addition, this method '\n",
        "          'will introduce unnecessary overheads which grow quadratically with '\n",
        "          'the max sequence length.'))\n",
        "  flags.DEFINE_bool(\n",
        "      name='enable_checkpointing',\n",
        "      default=True,\n",
        "      help=flags_core.help_wrap(\n",
        "          'Whether to do checkpointing during training. When running under '\n",
        "          'benchmark harness, we will avoid checkpointing.'))\n",
        "  flags.DEFINE_bool(\n",
        "      name='save_weights_only',\n",
        "      default=True,\n",
        "      help=flags_core.help_wrap(\n",
        "          'Only used when above `enable_checkpointing` is True. '\n",
        "          'If True, then only the model\\'s weights will be saved '\n",
        "          '(`model.save_weights(filepath)`), else the full model is saved '\n",
        "          '(`model.save(filepath)`)'))\n",
        "\n",
        "  flags_core.set_defaults(\n",
        "      data_dir='/tmp/translate_ende',\n",
        "      model_dir='/tmp/transformer_model',\n",
        "      batch_size=None)\n",
        "\n",
        "  # pylint: disable=unused-variable\n",
        "  @flags.multi_flags_validator(\n",
        "      ['bleu_source', 'bleu_ref'],\n",
        "      message='Both or neither --bleu_source and --bleu_ref must be defined.')\n",
        "  def _check_bleu_files(flags_dict):\n",
        "    return (flags_dict['bleu_source'] is None) == (\n",
        "        flags_dict['bleu_ref'] is None)\n",
        "\n",
        "  @flags.multi_flags_validator(\n",
        "      ['bleu_source', 'bleu_ref', 'vocab_file'],\n",
        "      message='--vocab_file must be defined if --bleu_source and --bleu_ref '\n",
        "      'are defined.')\n",
        "  def _check_bleu_vocab_file(flags_dict):\n",
        "    if flags_dict['bleu_source'] and flags_dict['bleu_ref']:\n",
        "      return flags_dict['vocab_file'] is not None\n",
        "    return True\n",
        "\n",
        "  # pylint: enable=unused-variable\n",
        "\n",
        "\n",
        "def get_callbacks():\n",
        "  \"\"\"Returns common callbacks.\"\"\"\n",
        "  callbacks = []\n",
        "  if FLAGS.enable_time_history:\n",
        "    time_callback = keras_utils.TimeHistory(\n",
        "        FLAGS.batch_size,\n",
        "        FLAGS.log_steps,\n",
        "        logdir=FLAGS.model_dir if FLAGS.enable_tensorboard else None)\n",
        "    callbacks.append(time_callback)\n",
        "\n",
        "  if FLAGS.enable_tensorboard:\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "        log_dir=FLAGS.model_dir)\n",
        "    callbacks.append(tensorboard_callback)\n",
        "\n",
        "  return callbacks\n",
        "\n",
        "\n",
        "def update_stats(history, stats, callbacks):\n",
        "  \"\"\"Normalizes and updates dictionary of stats.\n",
        "\n",
        "  Args:\n",
        "    history: Results of the training step.\n",
        "    stats: Dict with pre-existing training stats.\n",
        "    callbacks: a list of callbacks which might include a time history callback\n",
        "      used during keras.fit.\n",
        "  \"\"\"\n",
        "\n",
        "  if history and history.history:\n",
        "    train_hist = history.history\n",
        "    # Gets final loss from training.\n",
        "    stats['loss'] = float(train_hist['loss'][-1])\n",
        "\n",
        "  if not callbacks:\n",
        "    return\n",
        "\n",
        "  # Look for the time history callback which was used during keras.fit\n",
        "  for callback in callbacks:\n",
        "    if isinstance(callback, keras_utils.TimeHistory):\n",
        "      timestamp_log = callback.timestamp_log\n",
        "      stats['step_timestamp_log'] = timestamp_log\n",
        "      stats['train_finish_time'] = callback.train_finish_time\n",
        "      if len(timestamp_log) > 1:\n",
        "        stats['avg_exp_per_second'] = (\n",
        "            callback.batch_size * callback.log_steps *\n",
        "            (len(callback.timestamp_log) - 1) /\n",
        "            (timestamp_log[-1].timestamp - timestamp_log[0].timestamp))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHt3ovduTl63",
        "outputId": "a417990b-3af9-42a8-b38a-5f1d2df5275a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting attention_layer.py\n"
          ]
        }
      ],
      "source": [
        "#@title attention_layer\n",
        "%%writefile attention_layer.py\n",
        "# Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"Implementation of multiheaded attention and self-attention layers.\"\"\"\n",
        "import math\n",
        "import unittest\n",
        "from absl import logging\n",
        "import os \n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from official.nlp.modeling import layers\n",
        "\n",
        "index = 0\n",
        "query_logging_path = \"/content/drive/MyDrive/models/transformer/\"\n",
        "key_logging_path = \"/content/drive/MyDrive/models/transformer/\"\n",
        "value_logging_path = \"/content/drive/MyDrive/models/transformer/\"\n",
        "bias_logging_path = \"/content/drive/MyDrive/models/transformer/\"\n",
        "\n",
        "class Attention(tf.keras.layers.Layer):\n",
        "  \"\"\"Multi-headed attention layer.\"\"\"\n",
        "\n",
        "  def __init__(self, hidden_size, num_heads, attention_dropout):\n",
        "    \"\"\"Initialize Attention.\n",
        "\n",
        "    Args:\n",
        "      hidden_size: int, output dim of hidden layer.\n",
        "      num_heads: int, number of heads to repeat the same attention structure.\n",
        "      attention_dropout: float, dropout rate inside attention for training.\n",
        "    \"\"\"\n",
        "    if hidden_size % num_heads:\n",
        "      raise ValueError(\n",
        "          \"Hidden size ({}) must be divisible by the number of heads ({}).\"\n",
        "          .format(hidden_size, num_heads))\n",
        "\n",
        "    super(Attention, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_heads = num_heads\n",
        "    self.attention_dropout = attention_dropout\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    \"\"\"Builds the layer.\"\"\"\n",
        "    # Layers for linearly projecting the queries, keys, and values.\n",
        "    size_per_head = self.hidden_size // self.num_heads\n",
        "\n",
        "    def _glorot_initializer(fan_in, fan_out):\n",
        "      limit = math.sqrt(6.0 / (fan_in + fan_out))\n",
        "      return tf.keras.initializers.RandomUniform(minval=-limit, maxval=limit)\n",
        "\n",
        "    attention_initializer = _glorot_initializer(input_shape.as_list()[-1],\n",
        "                                                self.hidden_size)\n",
        "    self.query_dense_layer = layers.DenseEinsum(\n",
        "        output_shape=(self.num_heads, size_per_head),\n",
        "        kernel_initializer=attention_initializer,\n",
        "        use_bias=False,\n",
        "        name=\"query\")\n",
        "    self.key_dense_layer = layers.DenseEinsum(\n",
        "        output_shape=(self.num_heads, size_per_head),\n",
        "        kernel_initializer=attention_initializer,\n",
        "        use_bias=False,\n",
        "        name=\"key\")\n",
        "    self.value_dense_layer = layers.DenseEinsum(\n",
        "        output_shape=(self.num_heads, size_per_head),\n",
        "        kernel_initializer=attention_initializer,\n",
        "        use_bias=False,\n",
        "        name=\"value\")\n",
        "\n",
        "    output_initializer = _glorot_initializer(self.hidden_size, self.hidden_size)\n",
        "    self.output_dense_layer = layers.DenseEinsum(\n",
        "        output_shape=self.hidden_size,\n",
        "        num_summed_dimensions=2,\n",
        "        kernel_initializer=output_initializer,\n",
        "        use_bias=False,\n",
        "        name=\"output_transform\")\n",
        "    super(Attention, self).build(input_shape)\n",
        "\n",
        "  def get_config(self):\n",
        "    return {\n",
        "        \"hidden_size\": self.hidden_size,\n",
        "        \"num_heads\": self.num_heads,\n",
        "        \"attention_dropout\": self.attention_dropout,\n",
        "    }\n",
        "\n",
        "  def call(self,\n",
        "           query_input,\n",
        "           source_input,\n",
        "           bias,\n",
        "           training,\n",
        "           cache=None,\n",
        "           decode_loop_step=None):\n",
        "    \"\"\"Apply attention mechanism to query_input and source_input.\n",
        "\n",
        "    Args:\n",
        "      query_input: A tensor with shape [batch_size, length_query, hidden_size].\n",
        "      source_input: A tensor with shape [batch_size, length_source,\n",
        "        hidden_size].\n",
        "      bias: A tensor with shape [batch_size, 1, length_query, length_source],\n",
        "        the attention bias that will be added to the result of the dot product.\n",
        "      training: A bool, whether in training mode or not.\n",
        "      cache: (Used during prediction) A dictionary with tensors containing\n",
        "        results of previous attentions. The dictionary must have the items:\n",
        "            {\"k\": tensor with shape [batch_size, i, heads, dim_per_head],\n",
        "             \"v\": tensor with shape [batch_size, i, heads, dim_per_head]} where\n",
        "               i is the current decoded length for non-padded decode, or max\n",
        "               sequence length for padded decode.\n",
        "      decode_loop_step: An integer, step number of the decoding loop. Used only\n",
        "        for autoregressive inference on TPU.\n",
        "\n",
        "    Returns:\n",
        "      Attention layer output with shape [batch_size, length_query, hidden_size]\n",
        "    \"\"\"\n",
        "    # Linearly project the query, key and value using different learned\n",
        "    # projections. Splitting heads is automatically done during the linear\n",
        "    # projections --> [batch_size, length, num_heads, dim_per_head].\n",
        "    logging.info(\"Attention Layer Called!!!!!\")\n",
        "    global index\n",
        "    logging.info(\"index\" + str(index))\n",
        "    if not tf.executing_eagerly():\n",
        "      logging.info(\"Tensorflow runs in Graph Mode. TF is building the graph.\")\n",
        "    else:\n",
        "      logging.info(\"Tensorflow runs in dynamic execution mode now.\")\n",
        "    query = self.query_dense_layer(query_input)\n",
        "    key = self.key_dense_layer(source_input)\n",
        "    value = self.value_dense_layer(source_input)\n",
        "\n",
        "    if cache is not None:\n",
        "      # Combine cached keys and values with new keys and values.\n",
        "      if decode_loop_step is not None:\n",
        "        cache_k_shape = cache[\"k\"].shape.as_list()\n",
        "        indices = tf.reshape(\n",
        "            tf.one_hot(decode_loop_step, cache_k_shape[1], dtype=key.dtype),\n",
        "            [1, cache_k_shape[1], 1, 1])\n",
        "        key = cache[\"k\"] + key * indices\n",
        "        cache_v_shape = cache[\"v\"].shape.as_list()\n",
        "        indices = tf.reshape(\n",
        "            tf.one_hot(decode_loop_step, cache_v_shape[1], dtype=value.dtype),\n",
        "            [1, cache_v_shape[1], 1, 1])\n",
        "        value = cache[\"v\"] + value * indices\n",
        "      else:\n",
        "        key = tf.concat([tf.cast(cache[\"k\"], key.dtype), key], axis=1)\n",
        "        value = tf.concat([tf.cast(cache[\"v\"], value.dtype), value], axis=1)\n",
        "\n",
        "      # Update cache\n",
        "      cache[\"k\"] = key\n",
        "      cache[\"v\"] = value\n",
        "\n",
        "    # Scale query to prevent the dot product between query and key from growing\n",
        "    # too large.\n",
        "    depth = (self.hidden_size // self.num_heads)\n",
        "    query *= depth**-0.5\n",
        "\n",
        "    # Calculate dot product attention\n",
        "    logits = tf.einsum(\"BTNH,BFNH->BNFT\", key, query)\n",
        "    logits += bias\n",
        "    # Note that softmax internally performs math operations using float32\n",
        "    # for numeric stability. When training with float16, we keep the input\n",
        "    # and output in float16 for better performance.\n",
        "    weights = tf.nn.softmax(logits, name=\"attention_weights\")\n",
        "    # *********************************************************\n",
        "    #First Test without dropout layer\n",
        "    if training:\n",
        "      weights = tf.nn.dropout(weights, rate=self.attention_dropout)\n",
        "    #*********************************************************\n",
        "    attention_output = tf.einsum(\"BNFT,BTNH->BFNH\", weights, value)\n",
        "    end_time = time.time()\n",
        "\n",
        "    #fusion_output = self.fusion(query, key, bias, value)\n",
        "\n",
        "    # with open(logging_path, 'w') as f:\n",
        "    #   f.write(\"Execution time for original method: %f!\" % (end_time - start_time))\n",
        "    #   f.write(\"Execution time for new method: %f!\" % (new_end_time - new_start_time))\n",
        "    # f.close()\n",
        "\n",
        "    # if new_attention_output == attention_output:\n",
        "    #   logging.info(\"Fusion Successful!!!!!\")\n",
        "\n",
        "    # Run the outputs through another linear projection layer. Recombining heads\n",
        "    # is automatically done --> [batch_size, length, hidden_size]\n",
        "    string0 = tf.io.serialize_tensor(query)\n",
        "    tf.io.write_file(query_logging_path+\"logging_query\"+str(index)+\".txt\", string0)\n",
        "    string1 = tf.io.serialize_tensor(key)\n",
        "    tf.io.write_file(key_logging_path+\"logging_key\"+str(index)+\".txt\", string1)\n",
        "    string2 = tf.io.serialize_tensor(value)\n",
        "    tf.io.write_file(value_logging_path+\"logging_value\"+str(index)+\".txt\", string2)\n",
        "    string3 = tf.io.serialize_tensor(bias)\n",
        "    tf.io.write_file(bias_logging_path+\"logging_bias\"+str(index)+\".txt\", string3)\n",
        "    index += 1\n",
        "\n",
        "    attention_output = self.output_dense_layer(attention_output)\n",
        "\n",
        "    return attention_output\n",
        "\n",
        "  # Fusion Method\n",
        "  def fusion(self, query, key, bias, values):\n",
        "    \"\"\"Fuse Logit and Attend operators\n",
        "\n",
        "    Args:\n",
        "      query_input: query output from query_dense_layer with shape [batch_size, length, num_heads, dim_per_head].\n",
        "      key_input: key output from key_dense_layer with shape [batch_size, length, num_heads, dim_per_head].\n",
        "      value_input: value output from value_dense_layer with shape [batch_size, length, num_heads, dim_per_head].\n",
        "      bias_matrix: A tensor with shape [batch_size, 1, length_query, length_source],\n",
        "\n",
        "    Symbol meanings:\n",
        "      B : batch size\n",
        "      T : key_length == length_qeury\n",
        "      N : Head_num\n",
        "      H : dim_per_head\n",
        "      F : query_length == length_source (which also commented as )\n",
        "    Returns:\n",
        "      Attention layer output with shape [batch_size, length_query, hidden_size]\n",
        "\n",
        "      logits = tf.einsum(\"BTNH,BFNH->BNFT\", key, query)\n",
        "      attention_output = tf.einsum(\"BNFT,BTNH->BFNH\", weights, value)\n",
        "    \"\"\"\n",
        "    # Set the granularity as one for now\n",
        "    batch_granularity = 1\n",
        "    head_granularity = 2\n",
        "    element_granularity = 1\n",
        "\n",
        "    batch_size = 4096\n",
        "    input_length = 256\n",
        "\n",
        "    # Get shape of the matrix\n",
        "    if tf.executing_eagerly():\n",
        "      logging.info(\"Fusion works in the dynamic mode.\")\n",
        "      batch_size, source_length, head_num, dim = tf.shape(query).numpy()\n",
        "      _,key_length,_,_, = tf.shape(key).numpy()\n",
        "    else:\n",
        "      logging.info(\"Fusion works in the graph mode.\")\n",
        "      _, _, head_num, dim = query.get_shape().as_list()\n",
        "      source_length = input_length\n",
        "      key_length = input_length\n",
        "      return \n",
        "    #assert (head_granularity < head_num)\n",
        "    #Bias shape is different then what we expect\n",
        "    # bias = tf.reshape(bias, [-1, 1, 1, :])\n",
        "    logging.info(\"QUERY Shape\")\n",
        "    logging.info(query.shape)\n",
        "    logging.info(\"KEY Shape\")\n",
        "    logging.info(key.shape)\n",
        "    logging.info(\"BIAS Shape\")\n",
        "    logging.info(bias.shape)\n",
        "    logging.info(\"VALUES Shape\")\n",
        "    logging.info(values.shape)\n",
        "\n",
        "    #Use tf.Variables to pre-fix the shape of each tensor for graph build\n",
        "    output_from_unit_batch = tf.Variable(0., shape=tf.TensorShape(None))\n",
        "    output_from_unit_head = tf.Variable(0., shape=tf.TensorShape(None))\n",
        "    logit_output = tf.Variable(0., shape=tf.TensorShape(None))\n",
        "    key_source = tf.Variable(0., shape=tf.TensorShape(None))\n",
        "    result = tf.Variable(0., shape=tf.TensorShape(None))\n",
        "    value_source = tf.Variable(0., shape=tf.TensorShape(None))\n",
        "    attention_output = tf.Variable(0., shape=tf.TensorShape(None))\n",
        "    suboutput = tf.Variable(0., shape=tf.TensorShape(None))\n",
        "    output = tf.Variable(0., shape=tf.TensorShape(None))\n",
        "\n",
        "    # The outermost loop loops through batch_size, with granularity as stride\n",
        "    for batch in tf.range(0, batch_size, batch_granularity):\n",
        "      # The inner loop loops through head, with granularity as stride\n",
        "      for head in tf.range(0, head_num, head_granularity):\n",
        "        #output_from_unit_batch = tf.zeros([batch_granularity, source_length, head_granularity, dim])\n",
        "        # Check each bach inside each granularity\n",
        "        for unit_batch in tf.range(batch, batch + batch_granularity):\n",
        "          #output_from_unit_head = tf.zeros([source_length, head_granularity, dim])\n",
        "          # Check each head in the head granularity\n",
        "          for unit_head in tf.range(head, head + head_granularity):\n",
        "            #for unit_element in range(element):\n",
        "            #logit_output = tf.ones([source_length, key_length])\n",
        "            for loop in tf.range(2):\n",
        "              if loop == 0:\n",
        "                #iterate through query_length\n",
        "                for unit_source in tf.range(source_length):\n",
        "                  query_source = tf.Variable(0., shape=tf.TensorShape(None))\n",
        "                  query_source.assign(query[unit_batch, unit_source, unit_head, :])\n",
        "                  query_source = tf.reshape(query_source, [1, -1]) # Tensor with 1 * 10 (unit_source_length * dim_per_head)\n",
        "                  key_source.assign(key[unit_batch, :, unit_head, :]) # Tensor with 64 * 10 (key length * dim_per_head)\n",
        "                  #Result is F(1) * T matrix\n",
        "                  result.assign(tf.matmul(query_source, tf.transpose(key_source))) # Tensor with 1 * 64 size (unit_source_length * key_length)\n",
        "                  #Broadcasting here\n",
        "                  result.assign(bias[unit_batch, :, :, unit_source] + result)\n",
        "                  #Row granularity\n",
        "                  result.assign(tf.nn.softmax(result, name=\"attention_weights\", axis=-1))\n",
        "                  if unit_source == 0:\n",
        "                    logit_output.assign(result)\n",
        "                  else:\n",
        "                    logit_output.assign(tf.concat([logit_output, result], 0))\n",
        "              else:\n",
        "                value_source.assign(values[unit_batch, :, unit_head, :]) # Tensor with 64 * 10 (T * H) query_length * dim_per_head\n",
        "                # tf.cast(logit_output, dtype=float)\n",
        "                # tf.cast(value_source, dtype=float)\n",
        "                attention_output.assign(tf.matmul(logit_output, value_source)) #Matrix with size F * H\n",
        "            if unit_head == head:\n",
        "              dummy_attention = tf.convert_to_tensor(attention_output)\n",
        "              dummy_attention = tf.expand_dims(dummy_attention, axis=1)\n",
        "              output_from_unit_head.assign(dummy_attention)\n",
        "            else:\n",
        "              dummy_attention = tf.convert_to_tensor(attention_output)\n",
        "              dummy_attention = tf.expand_dims(dummy_attention, axis=1)\n",
        "              output_from_unit_head.assign(tf.concat([output_from_unit_head, dummy_attention], 1))\n",
        "          #Size now is F*N*H (output_from_unit_head)\n",
        "          if unit_batch == batch:\n",
        "            dummy_unit_head = tf.convert_to_tensor(output_from_unit_head)\n",
        "            dummy_unit_head = tf.expand_dims(dummy_unit_head, axis=0)\n",
        "            output_from_unit_batch.assign(dummy_unit_head)\n",
        "          else:\n",
        "            dummy_unit_head = tf.convert_to_tensor(output_from_unit_head)\n",
        "            dummy_unit_head = tf.expand_dims(dummy_unit_head, axis=0)\n",
        "            output_from_unit_batch.assign(tf.concat([output_from_unit_batch, dummy_unit_head], 0))\n",
        "        # Size now is BFNH (output from unit batch)\n",
        "        logging.info(\"Exit from lower granularity\")\n",
        "        if head == 0:\n",
        "          suboutput.assign(output_from_unit_batch)\n",
        "        else:\n",
        "          suboutput.assign(tf.concat([suboutput, output_from_unit_batch], 2))\n",
        "      logging.info(\"Last step merging batch!!\")\n",
        "      logging.info(tf.convert_to_tensor(suboutput).shape)\n",
        "      if batch == 0:\n",
        "        output.assign(suboutput)\n",
        "      else:\n",
        "        output.assign(tf.concat([output, suboutput], 0))\n",
        "      logging.info(\"Merge Batch Out\")\n",
        "      logging.info(tf.convert_to_tensor(output).shape)\n",
        "    logging.info(\"Fusion finishes!!!\")\n",
        "    return tf.convert_to_tensor(output)\n",
        "\n",
        "    # Design choices\n",
        "    # 1. Now totally mute the graph build execution : can write two totally different code\n",
        "    # 2. Reset tf Variable everytime or create a dummy tensor\n",
        "\n",
        "class SelfAttention(Attention):\n",
        "  \"\"\"Multiheaded self-attention layer.\"\"\"\n",
        "\n",
        "  def call(self,\n",
        "           query_input,\n",
        "           bias,\n",
        "           training,\n",
        "           cache=None,\n",
        "           decode_loop_step=None):\n",
        "    return super(SelfAttention, self).call(query_input, query_input, bias,\n",
        "                                           training, cache, decode_loop_step)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcXmHV0HS2g-"
      },
      "outputs": [],
      "source": [
        "# To eliminate backward propagation, another way is to specify tf.no_gradient(zip(grads, tvars))\n",
        "! pip uninstall tf-models-official"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKt_IplauKp1"
      },
      "outputs": [],
      "source": [
        "#Tony's run\n",
        "! rm $MODEL_DIR/checkpoint\n",
        "! python3 transformer_main.py --data_dir=$DATA_DIR --model_dir=$MODEL_DIR \\\n",
        "    --vocab_file=$VOCAB_FILE --param_set=$PARAM_SET \\\n",
        "    --train_steps=3 --steps_between_evals=1 \\\n",
        "    --batch_size=4096 --max_length=64 \\\n",
        "    --bleu_source=$DATA_DIR/newstest2014.en \\\n",
        "    --bleu_ref=$DATA_DIR/newstest2014.de \\\n",
        "    --num_gpus=1 \\\n",
        "    --enable_time_history=false \\\n",
        "    --decode_batch_size=32 \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0--Ou0_0ldAO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28a15978-f8ce-4d32-d585-94ac3ec5217b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#Swetha/Harsh - check if model is running with enable checkpoint \n",
        "! rm $MODEL_DIR/checkpoint\n",
        "!python3 transformer_main.py --data_dir=$DATA_DIR --model_dir=$MODEL_DIR \\\n",
        "    --vocab_file=$VOCAB_FILE --param_set=$PARAM_SET \\\n",
        "    --train_steps=10 \\ --max_length=64 \\\n",
        "    --bleu_source=$DATA_DIR/newstest2014.en \\\n",
        "    --bleu_ref=$DATA_DIR/newstest2014.de \\\n",
        "    --num_gpus=1 \\\n",
        "    --enable_time_history=false \\\n",
        "    --decode_batch_size=32 \\\n",
        "    --enable_checkpointing = true \\\n",
        "    --save_weights_only = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlyqC9_q77ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bab62619-5c39-4248-f914-6056434740fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/drive/MyDrive/models/transformer/model_big/checkpoint': No such file or directory\n",
            "WARNING:tensorflow:From /content/models/official/nlp/transformer/transformer.py:36: experimental_run_functions_eagerly (from tensorflow.python.eager.def_function) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.run_functions_eagerly` instead of the experimental version.\n",
            "2021-12-15 04:31:07.915895: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "I1215 04:31:07.919347 140588040787840 mirrored_strategy.py:376] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "I1215 04:31:07.920098 140588040787840 transformer_main.py:174] Running transformer with num_gpus = 1\n",
            "I1215 04:31:07.920301 140588040787840 transformer_main.py:178] For training, using distribution strategy: <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7fdcafbebed0>\n",
            "I1215 04:31:07.921984 140588040787840 transformer.py:39] Create Transformer Model!!!!\n",
            "I1215 04:31:09.120376 140588040787840 api.py:447] Transformer Task\n",
            "I1215 04:31:10.536424 140588040787840 api.py:447] False\n",
            "I1215 04:31:10.772788 140588040787840 api.py:447] Start Encoding!!!!!\n",
            "I1215 04:31:11.287160 140588040787840 transformer.py:403] Encoder attention layer added!!!!\n",
            "I1215 04:31:12.217084 140588040787840 api.py:447] Start Decoding!!!\n",
            "I1215 04:31:12.350745 140588040787840 transformer.py:476] Decoder attention layer added!!!!\n",
            "W1215 04:31:12.924607 140588040787840 performance.py:27] `use_graph_rewrite` is deprecated inside `configure_optimizer`. Please remove the usage.\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1215 04:31:12.935577 140588040787840 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1215 04:31:12.937098 140588040787840 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1215 04:31:12.939565 140588040787840 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1215 04:31:12.940583 140588040787840 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " inputs (InputLayer)            [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " targets (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " transformer_v2 (Transformer)   (None, None, 33708)  63901696    ['inputs[0][0]',                 \n",
            "                                                                  'targets[0][0]']                \n",
            "                                                                                                  \n",
            " logits (Lambda)                (None, None, 33708)  0           ['transformer_v2[0][0]']         \n",
            "                                                                                                  \n",
            " tf.compat.v1.shape (TFOpLambda  (3,)                0           ['logits[0][0]']                 \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.compat.v1.shape_1 (TFOpLamb  (2,)                0           ['targets[0][0]']                \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  ()                  0           ['tf.compat.v1.shape[0][0]']     \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  ()                  0           ['tf.compat.v1.shape_1[0][0]']   \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " tf.math.maximum (TFOpLambda)   ()                   0           ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 , 'tf.__operators__.getitem_1[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " tf.math.subtract_1 (TFOpLambda  ()                  0           ['tf.math.maximum[0][0]',        \n",
            " )                                                                'tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " tf.compat.v1.pad_1 (TFOpLambda  (None, None)        0           ['targets[0][0]',                \n",
            " )                                                                'tf.math.subtract_1[0][0]']     \n",
            "                                                                                                  \n",
            " tf.cast (TFOpLambda)           (None, None)         0           ['tf.compat.v1.pad_1[0][0]']     \n",
            "                                                                                                  \n",
            " tf.math.subtract (TFOpLambda)  ()                   0           ['tf.math.maximum[0][0]',        \n",
            "                                                                  'tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.one_hot (TFOpLambda)        (None, None, 33708)  0           ['tf.cast[0][0]']                \n",
            "                                                                                                  \n",
            " tf.compat.v1.pad (TFOpLambda)  (None, None, 33708)  0           ['logits[0][0]',                 \n",
            "                                                                  'tf.math.subtract[0][0]']       \n",
            "                                                                                                  \n",
            " tf.compat.v1.nn.softmax_cross_  (None, None)        0           ['tf.one_hot[0][0]',             \n",
            " entropy_with_logits_v2 (TFOpLa                                   'tf.compat.v1.pad[0][0]']       \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " tf.math.not_equal (TFOpLambda)  (None, None)        0           ['tf.compat.v1.pad_1[0][0]']     \n",
            "                                                                                                  \n",
            " tf.math.subtract_2 (TFOpLambda  (None, None)        0           ['tf.compat.v1.nn.softmax_cross_e\n",
            " )                                                               ntropy_with_logits_v2[0][0]']    \n",
            "                                                                                                  \n",
            " tf.cast_1 (TFOpLambda)         (None, None)         0           ['tf.math.not_equal[0][0]']      \n",
            "                                                                                                  \n",
            " tf.math.multiply (TFOpLambda)  (None, None)         0           ['tf.math.subtract_2[0][0]',     \n",
            "                                                                  'tf.cast_1[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.reduce_sum (TFOpLambda  ()                  0           ['tf.math.multiply[0][0]']       \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.math.reduce_sum_1 (TFOpLamb  ()                  0           ['tf.cast_1[0][0]']              \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " tf.math.truediv (TFOpLambda)   ()                   0           ['tf.math.reduce_sum[0][0]',     \n",
            "                                                                  'tf.math.reduce_sum_1[0][0]']   \n",
            "                                                                                                  \n",
            " add_loss (AddLoss)             ()                   0           ['tf.math.truediv[0][0]']        \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 63,901,696\n",
            "Trainable params: 63,901,696\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py:4527: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  \"Even though the `tf.config.experimental_run_functions_eagerly` \"\n",
            "WARNING:tensorflow:From /content/models/official/nlp/transformer/data_pipeline.py:189: group_by_window (from tensorflow.python.data.experimental.ops.grouping) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.group_by_window(...)`.\n",
            "W1215 04:31:13.225488 140588040787840 deprecation.py:347] From /content/models/official/nlp/transformer/data_pipeline.py:189: group_by_window (from tensorflow.python.data.experimental.ops.grouping) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.group_by_window(...)`.\n",
            "I1215 04:31:13.351673 140588040787840 transformer_main.py:297] Start train iteration at global step:0\n",
            "2021-12-15 04:31:13.368821: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"GroupByWindowDataset/_19\"\n",
            "op: \"GroupByWindowDataset\"\n",
            "input: \"FilterDataset/_16\"\n",
            "input: \"Placeholder/_17\"\n",
            "input: \"Placeholder/_18\"\n",
            "attr {\n",
            "  key: \"Tkey_func_other_arguments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"Treduce_func_other_arguments\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"Twindow_size_func_other_arguments\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"_cardinality\"\n",
            "  value {\n",
            "    i: -2\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"key_func\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_group_by_window_key_func_wrapper_1386\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"metadata\"\n",
            "  value {\n",
            "    s: \"\\n\\026GroupByWindowDataset:6\"\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT64\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"reduce_func\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_group_by_window_batching_fn_1406\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"window_size_func\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_group_by_window_window_size_func_wrapper_1419\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n",
            "W1215 04:31:13.737281 140588040787840 mirrored_run.py:95] Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n",
            "I1215 04:31:13.744470 140583606490880 transformer.py:121] Transformer Task\n",
            "I1215 04:31:13.744843 140583606490880 transformer.py:122] True\n",
            "I1215 04:31:13.748997 140583606490880 transformer.py:166] Start Encoding!!!!!\n",
            "I1215 04:31:14.118921 140583606490880 transformer.py:201] Start Decoding!!!\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1215 04:31:15.376331 140588040787840 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1215 04:31:15.380603 140588040787840 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1215 04:31:15.381776 140588040787840 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "1/1 [==============================] - ETA: 0s - loss: 18.87182021-12-15 04:31:15.414220: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 138067968 exceeds 10% of free system memory.\n",
            "2021-12-15 04:31:15.595449: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 138067968 exceeds 10% of free system memory.\n",
            "2021-12-15 04:31:15.777751: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 138067968 exceeds 10% of free system memory.\n",
            "1/1 [==============================] - 6s 6s/step - loss: 18.8718\n",
            "I1215 04:31:19.452875 140588040787840 transformer_main.py:357] Train history: {'loss': [18.87181282043457]}\n",
            "I1215 04:31:19.453046 140588040787840 transformer_main.py:360] All done\n",
            "\n",
            "I1215 04:31:19.453144 140588040787840 transformer_main.py:363] Checkpointing is enabled\n",
            "\n",
            "I1215 04:31:19.453227 140588040787840 transformer_main.py:364] End train iteration at global step:1\n",
            "I1215 04:31:19.453328 140588040787840 transformer_main.py:297] Start train iteration at global step:1\n",
            "2021-12-15 04:31:19.469883: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"GroupByWindowDataset/_19\"\n",
            "op: \"GroupByWindowDataset\"\n",
            "input: \"FilterDataset/_16\"\n",
            "input: \"Placeholder/_17\"\n",
            "input: \"Placeholder/_18\"\n",
            "attr {\n",
            "  key: \"Tkey_func_other_arguments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"Treduce_func_other_arguments\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"Twindow_size_func_other_arguments\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"_cardinality\"\n",
            "  value {\n",
            "    i: -2\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"key_func\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_group_by_window_key_func_wrapper_1386\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"metadata\"\n",
            "  value {\n",
            "    s: \"\\n\\026GroupByWindowDataset:6\"\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT64\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"reduce_func\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_group_by_window_batching_fn_1406\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"window_size_func\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_group_by_window_window_size_func_wrapper_1419\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1215 04:31:19.571299 140588040787840 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1215 04:31:19.573019 140588040787840 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "Epoch 2/2\n",
            "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n",
            "W1215 04:31:19.776686 140588040787840 mirrored_run.py:95] Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n",
            "I1215 04:31:19.783013 140583606490880 transformer.py:121] Transformer Task\n",
            "I1215 04:31:19.783289 140583606490880 transformer.py:122] True\n",
            "I1215 04:31:19.785394 140583606490880 transformer.py:166] Start Encoding!!!!!\n",
            "I1215 04:31:19.842799 140583606490880 transformer.py:201] Start Decoding!!!\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1215 04:31:21.002356 140588040787840 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "1/1 [==============================] - ETA: 0s - loss: 18.97032021-12-15 04:31:21.036246: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 138067968 exceeds 10% of free system memory.\n",
            "2021-12-15 04:31:21.112569: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 138067968 exceeds 10% of free system memory.\n",
            "1/1 [==============================] - 5s 5s/step - loss: 18.9703\n",
            "I1215 04:31:24.683346 140588040787840 transformer_main.py:357] Train history: {'loss': [18.970260620117188]}\n",
            "I1215 04:31:24.683561 140588040787840 transformer_main.py:360] All done\n",
            "\n",
            "I1215 04:31:24.683681 140588040787840 transformer_main.py:363] Checkpointing is enabled\n",
            "\n",
            "I1215 04:31:24.683778 140588040787840 transformer_main.py:364] End train iteration at global step:2\n",
            "I1215 04:31:24.683895 140588040787840 transformer_main.py:297] Start train iteration at global step:2\n",
            "2021-12-15 04:31:24.707966: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"GroupByWindowDataset/_19\"\n",
            "op: \"GroupByWindowDataset\"\n",
            "input: \"FilterDataset/_16\"\n",
            "input: \"Placeholder/_17\"\n",
            "input: \"Placeholder/_18\"\n",
            "attr {\n",
            "  key: \"Tkey_func_other_arguments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"Treduce_func_other_arguments\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"Twindow_size_func_other_arguments\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"_cardinality\"\n",
            "  value {\n",
            "    i: -2\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"key_func\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_group_by_window_key_func_wrapper_1386\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"metadata\"\n",
            "  value {\n",
            "    s: \"\\n\\026GroupByWindowDataset:6\"\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT64\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"reduce_func\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_group_by_window_batching_fn_1406\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"window_size_func\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_group_by_window_window_size_func_wrapper_1419\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "Epoch 3/3\n",
            "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n",
            "W1215 04:31:25.029830 140588040787840 mirrored_run.py:95] Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n",
            "I1215 04:31:25.036327 140583606490880 transformer.py:121] Transformer Task\n",
            "I1215 04:31:25.036489 140583606490880 transformer.py:122] True\n",
            "I1215 04:31:25.037207 140583606490880 transformer.py:166] Start Encoding!!!!!\n",
            "I1215 04:31:25.078613 140583606490880 transformer.py:201] Start Decoding!!!\n",
            "1/1 [==============================] - 18s 18s/step - loss: 18.9365\n",
            "I1215 04:31:45.298323 140588040787840 transformer_main.py:357] Train history: {'loss': [18.93647575378418]}\n",
            "I1215 04:31:45.298640 140588040787840 transformer_main.py:360] All done\n",
            "\n",
            "I1215 04:31:45.298789 140588040787840 transformer_main.py:363] Checkpointing is enabled\n",
            "\n",
            "I1215 04:31:45.298930 140588040787840 transformer_main.py:364] End train iteration at global step:3\n",
            "I1215 04:31:45.299096 140588040787840 transformer_main.py:297] Start train iteration at global step:3\n",
            "2021-12-15 04:31:45.336524: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"GroupByWindowDataset/_19\"\n",
            "op: \"GroupByWindowDataset\"\n",
            "input: \"FilterDataset/_16\"\n",
            "input: \"Placeholder/_17\"\n",
            "input: \"Placeholder/_18\"\n",
            "attr {\n",
            "  key: \"Tkey_func_other_arguments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"Treduce_func_other_arguments\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"Twindow_size_func_other_arguments\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"_cardinality\"\n",
            "  value {\n",
            "    i: -2\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"key_func\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_group_by_window_key_func_wrapper_1386\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"metadata\"\n",
            "  value {\n",
            "    s: \"\\n\\026GroupByWindowDataset:6\"\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT64\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"reduce_func\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_group_by_window_batching_fn_1406\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"window_size_func\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_group_by_window_window_size_func_wrapper_1419\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "Epoch 4/4\n",
            "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n",
            "W1215 04:31:46.038381 140588040787840 mirrored_run.py:95] Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n",
            "I1215 04:31:46.054201 140583606490880 transformer.py:121] Transformer Task\n",
            "I1215 04:31:46.054450 140583606490880 transformer.py:122] True\n",
            "I1215 04:31:46.061401 140583606490880 transformer.py:166] Start Encoding!!!!!\n",
            "I1215 04:31:46.192780 140583606490880 transformer.py:201] Start Decoding!!!\n",
            "1/1 [==============================] - 14s 14s/step - loss: 18.8666\n",
            "I1215 04:31:59.711386 140588040787840 transformer_main.py:357] Train history: {'loss': [18.866596221923828]}\n",
            "I1215 04:31:59.711596 140588040787840 transformer_main.py:360] All done\n",
            "\n",
            "I1215 04:31:59.711717 140588040787840 transformer_main.py:363] Checkpointing is enabled\n",
            "\n",
            "I1215 04:31:59.711827 140588040787840 transformer_main.py:364] End train iteration at global step:4\n"
          ]
        }
      ],
      "source": [
        "#Swetha - running model with default configuration and file - use this one \n",
        "! rm $MODEL_DIR/checkpoint\n",
        "! python3 transformer_main.py --data_dir=$DATA_DIR --model_dir=$MODEL_DIR \\\n",
        "    --vocab_file=$VOCAB_FILE --param_set=$PARAM_SET \\\n",
        "    --train_steps=4 --steps_between_evals=1 \\\n",
        "    --batch_size=4096 --max_length=64 \\\n",
        "    --bleu_source=$DATA_DIR/newstest2014.en \\\n",
        "    --bleu_ref=$DATA_DIR/newstest2014.de \\\n",
        "    --num_gpus=1 \\\n",
        "    --enable_time_history=false \\\n",
        "    --decode_batch_size=32 \\\n",
        "    --enable_checkpointing=true \\\n",
        "    --save_weights_only=false "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iftHnVrQIpbk",
        "outputId": "d4507499-22e4-47f6-d818-7a882c896c31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "NOTE: Using experimental fast data loading logic. To disable, pass\n",
            "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
            "    https://github.com/tensorflow/tensorboard/issues/4784\n",
            "\n",
            "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
            "TensorBoard 2.7.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "! tensorboard --logdir=$MODEL_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eV3wB-V_X54Q",
        "outputId": "c26217b0-ddd0-4333-c6e4-895fce392b23"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([81, 49, 16, 64])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "query_logging_path = \"/content/drive/MyDrive/models/transformer/logging_query.txt\"\n",
        "key_logging_path = \"/content/drive/MyDrive/models/transformer/logging_key.txt\"\n",
        "query_file = tf.io.read_file(query_logging_path)\n",
        "key_file = tf.io.read_file(key_logging_path)\n",
        "query = tf.io.parse_tensor(query_file, out_type=tf.float32)\n",
        "key = tf.io.parse_tensor(key_file, out_type=tf.float32)\n",
        "query.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dh22ENMJYoC-"
      },
      "outputs": [],
      "source": [
        "key == query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8M4hA36IOSM3"
      },
      "outputs": [],
      "source": [
        "t = tf.zeros([2, 2])\n",
        "shape = tf.shape(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wheGCRRRPqQi",
        "outputId": "3ae4c746-3604-4757-8048-d1289bfde930"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([54, 1, 1, 54])"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "test = tf.zeros([1, 1, 54, 54])\n",
        "test = tf.reshape(test, [-1, 1, 1, test.shape.as_list()[3]])\n",
        "test.shape"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Model_Run.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
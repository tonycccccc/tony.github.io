{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinalDemo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8PilO4Tpt4X",
        "outputId": "9d959da7-c2ea-4473-d12a-54698cdf8fa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#import library\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import json\n",
        "#mount the google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#check the runtime type\n",
        "! nvidia-smi\n",
        "\"\"\"\n",
        "FLAT Dataflow Implementation:\n",
        "\"\"\"\n",
        "def FlatDataflow(loop_num, query, key, value, bias, batch_granularity_level1=1, batch_granularity_level2=1, head_granularity_level1=16,\n",
        "                 head_granularity_level2=8, length_granularity_level=64):\n",
        "  if (tf.config.list_physical_devices('GPU')):\n",
        "    memory_before = tf.config.experimental.get_memory_info('GPU:0')\n",
        "    print(\"Iteration %d: Before running, reset the memory! Memory peak: %f; Memory current: %f\"%(loop_num, memory_before['peak'], memory_before['current']))\n",
        "    batch_size, source_length, head_num, dim = tf.shape(query).numpy()\n",
        "    #since bias matrix shape is not fixed from time to time, randomly select a number here\n",
        "    #biasValue = bias[np.random.randint(tf.reshape(bias, [-1, 1]).size())]\n",
        "    bias_value = bias[0, 0, 0, 0]\n",
        "    for batch in tf.range(0, batch_size, batch_granularity_level1):\n",
        "      for head in tf.range(0, head_num, head_granularity_level1):\n",
        "        batch_termination = batch + batch_granularity_level1 if batch + batch_granularity_level1 <= batch_size else batch_size\n",
        "        for unit_batch in tf.range(batch, batch_termination, batch_granularity_level2):\n",
        "          if (batch_granularity_level2 != 1):\n",
        "            #Lowest granularity is batch level\n",
        "            end_batch = unit_batch + batch_granularity_level2 if unit_batch + batch_granularity_level2 <= batch_termination else batch_termination\n",
        "            query_source = tf.gather(query[:, :, :, :], indices=tf.range(unit_batch, end_batch), axis=0)\n",
        "            print(query_source.shape)\n",
        "            key_source = tf.gather(key[:, :, :, :], indices=tf.range(unit_batch, end_batch), axis=0)\n",
        "            value_source = tf.gather(value[:, :, :, :], indices=tf.range(unit_batch, end_batch), axis=0)\n",
        "            result = tf.einsum(\"BTNH, BFNH->BNFT\", key_source, query_source)\n",
        "            result += bias_value\n",
        "            result = tf.nn.softmax(result, name=\"attention_weights\")\n",
        "            result = tf.nn.dropout(result, rate=0.4)\n",
        "            attention_output = tf.einsum(\"BNFT,BTNH->BFNH\", result, value_source)\n",
        "          else:\n",
        "            head_termination = head + head_granularity_level1 if head + head_granularity_level1 <= head_num else head_num\n",
        "            for unit_head in tf.range(head, head_termination, batch_granularity_level2):\n",
        "              #Lowest granularity is head level\n",
        "              if (head_granularity_level2 != 1):\n",
        "                end_head = unit_head + head_granularity_level2 if unit_head + head_granularity_level2 <= head_termination else head_termination\n",
        "                query_source = tf.gather(query[unit_batch, :, :, :], indices=tf.range(unit_head, end_head), axis=1)\n",
        "                key_source = tf.gather(key[unit_batch, :, :, :], indices=tf.range(unit_head, end_head), axis=1)\n",
        "                value_source = tf.gather(value[unit_batch, :, :, :], indices=tf.range(unit_head, end_head), axis=1)\n",
        "                result = tf.einsum(\"TNH, FNH->NFT\", key_source, query_source)\n",
        "                result += bias_value\n",
        "                result = tf.nn.softmax(result, name=\"attention_weights\")\n",
        "                result = tf.nn.dropout(result, rate=0.4)\n",
        "                logit = tf.einsum(\"NFT,TNH->FNH\", result, value_source)\n",
        "                if unit_head == head:\n",
        "                  attention_output = logit\n",
        "                else:\n",
        "                  attention_output = tf.concat([attention_output, logit], axis=1)\n",
        "              else:\n",
        "                #Lowest granularity is length level\n",
        "                for length in tf.range(0, source_length, length_granularity_level):\n",
        "                  end_length = length + length_granularity_level if length + length_granularity_level <= source_length else source_length\n",
        "                  query_source = tf.gather(query[unit_batch, :, unit_head, :], indices=tf.range(length, end_length), axis=0)\n",
        "                  key_source = key[unit_batch, :, unit_head, :]\n",
        "                  result = tf.einsum(\"TH, FH->FT\", key_source, query_source)\n",
        "                  result += bias_value\n",
        "                  result = tf.nn.softmax(result, name=\"attention_weights\")\n",
        "                  result = tf.nn.dropout(result, rate=0.4)\n",
        "                  if length == 0:\n",
        "                    lengthOutput = result\n",
        "                  else:\n",
        "                    lengthOutput = tf.concat([lengthOutput, result], axis=0)\n",
        "                value_source = value[unit_batch, :, unit_head, :]\n",
        "                lengthRes = tf.einsum(\"FT,TH->FH\", lengthOutput, value_source)\n",
        "                lengthRes = tf.expand_dims(lengthRes, axis=1)\n",
        "                if (unit_head == head):\n",
        "                  attention_output = lengthRes\n",
        "                else:\n",
        "                  attention_output = tf.concat([attention_output, logit], axis=1)\n",
        "            attention_output = tf.expand_dims(attention_output, axis=0)\n",
        "          if unit_batch == batch:\n",
        "            output_from_unit_batch = attention_output\n",
        "          else:\n",
        "            output_from_unit_batch = tf.concat([output_from_unit_batch, attention_output], 0)\n",
        "        if head == 0:\n",
        "          suboutput = output_from_unit_batch\n",
        "        else:\n",
        "          suboutput = tf.concat([suboutput, output_from_unit_batch], 2)\n",
        "      if batch == 0:\n",
        "        output = suboutput\n",
        "      else:\n",
        "        output = tf.concat([output, suboutput], 0)\n",
        "    stoptime = time.time()\n",
        "    memory_after = tf.config.experimental.get_memory_info('GPU:0')\n",
        "    print(\"Iteration %d: After running! Memory peak: %f; Memory current: %f\"%(loop_num, memory_after['peak'], memory_after['current']))\n",
        "    return (memory_before['peak'], memory_before['current'], memory_after['peak'], memory_after['current'], stoptime)"
      ],
      "metadata": {
        "id": "-TmRp4QVwLRo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c02cb544-8ce1-4404-e1a6-78f7073b39cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Dec 15 15:48:56 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#read the path to the target directory under user's google drive (modify the path /content/drive/MyDrive/path/to/ here before running)\n",
        "query_file = glob.glob(\"/content/drive/MyDrive/models/transformer/Test/logging_query*.txt\")\n",
        "key_file = glob.glob(\"/content/drive/MyDrive/models/transformer/Test/logging_key*.txt\")\n",
        "value_file = glob.glob(\"/content/drive/MyDrive/models/transformer/Test/logging_value*.txt\")\n",
        "bias_file = glob.glob(\"/content/drive/MyDrive/models/transformer/Test/logging_bias*.txt\")\n",
        "#read the file and parse it to tensor before sending to the flat function\n",
        "peak = []\n",
        "curr = []\n",
        "t = []\n",
        "#ask for user input\n",
        "batch1 = int(input(\"Enter Batch Granularity Level1:\"))\n",
        "batch2 = int(input(\"Enter Batch Granularity Level2:\"))\n",
        "head1 = int(input(\"Enter Head Granularity Level1:\"))\n",
        "head2 = int(input(\"Enter Head Granularity Level2:\"))\n",
        "length = int(input(\"Enter Length Granularity Level:\"))\n",
        "tf.config.experimental.reset_memory_stats('GPU:0')\n",
        "start_time = time.time()\n",
        "#write result to a temporary file\n",
        "path = \"/content/drive/MyDrive/models/transformer/result.txt\"\n",
        "f = open(path, 'a')\n",
        "grastr = \"%d*%d*%d*%d*%d \" % (batch1, batch2, head1, head2, length)\n",
        "for idx, file in enumerate(query_file):\n",
        "  query = tf.io.parse_tensor(tf.io.read_file(file), out_type=tf.float32)\n",
        "  key = tf.io.parse_tensor(tf.io.read_file(file), out_type=tf.float32)\n",
        "  value = tf.io.parse_tensor(tf.io.read_file(file), out_type=tf.float32)\n",
        "  bias = tf.io.parse_tensor(tf.io.read_file(file), out_type=tf.float32)\n",
        "  print(query.shape)\n",
        "  peakOld, currOld, peakCurr, currCurr, stoptime = FlatDataflow(idx, query, key, value, bias, batch_granularity_level1=batch1, batch_granularity_level2=batch2,\n",
        "                                                      head_granularity_level1=head1, head_granularity_level2=head2, length_granularity_level=length)\n",
        "  peak.append(peakCurr)\n",
        "  curr.append(currCurr)\n",
        "  t.append(stoptime - start_time)\n",
        "dir = {\"granularity\": grastr, \"peak\": sum(peak)/len(peak), \"curr\": sum(curr)/len(curr), \"time\": t[-1]/len(t)}\n",
        "f.write(json.dumps(dir))\n",
        "f.write(\"\\n\")\n",
        "f.close()\n",
        "iteration = np.arange(len(peak))\n",
        "plt.plot(iteration,peak,'r',label=\"Peak\")\n",
        "plt.plot(iteration,curr,'b',label='Current')\n",
        "plt.legend()\n",
        "plt.title(\"Peak&Current Memory Usage\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        },
        "id": "IxBEDh_RsoWU",
        "outputId": "d4ba82c3-6d5f-4d8c-e138-d4dbf3bc2301"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter Batch Granularity Level1:1\n",
            "Enter Batch Granularity Level2:1\n",
            "Enter Head Granularity Level1:64\n",
            "Enter Head Granularity Level2:32\n",
            "Enter Length Granularity Level:64\n",
            "(1, 4096, 16, 64)\n",
            "Iteration 0: Before running, reset the memory! Memory peak: 1280.000000; Memory current: 1280.000000\n",
            "Iteration 0: After running! Memory peak: 5754587136.000000; Memory current: 336592640.000000\n",
            "(1, 4096, 16, 64)\n",
            "Iteration 1: Before running, reset the memory! Memory peak: 5754587136.000000; Memory current: 67110656.000000\n",
            "Iteration 1: After running! Memory peak: 5821696000.000000; Memory current: 340786944.000000\n",
            "(1, 4096, 16, 64)\n",
            "Iteration 2: Before running, reset the memory! Memory peak: 5821696000.000000; Memory current: 33556224.000000\n",
            "Iteration 2: After running! Memory peak: 5821696000.000000; Memory current: 340786944.000000\n",
            "(1, 4096, 16, 64)\n",
            "Iteration 3: Before running, reset the memory! Memory peak: 5821696000.000000; Memory current: 16779008.000000\n",
            "Iteration 3: After running! Memory peak: 5821696000.000000; Memory current: 346029824.000000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Peak&Current Memory Usage')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(path, 'r')\n",
        "file_contents = f.read()\n",
        "print (file_contents)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "1zv-DMw6QjTd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "008af93d-938e-41fe-f71b-7a1c5d3f09fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"granularity\": \"1*1*1*1*32 \", \"peak\": 144640.0, \"curr\": 85760.0, \"time\": 0.707266092300415}\n",
            "{\"granularity\": \"1*1*1048*512*64 \", \"peak\": 111360.0, \"curr\": 85248.0, \"time\": 0.5663280487060547}\n",
            "{\"granularity\": \"16384*16384*16484*1*1 \", \"peak\": 166144.0, \"curr\": 133376.0, \"time\": 0.2881624698638916}\n",
            "{\"granularity\": \"1*1*4096*2048*64 \", \"peak\": 143616.0, \"curr\": 86016.0, \"time\": 0.6527509689331055}\n",
            "{\"granularity\": \"10*10*1*1*1 \", \"peak\": 112896.0, \"curr\": 88576.0, \"time\": 0.29667139053344727}\n",
            "{\"granularity\": \"10*10*1*1*1 \", \"peak\": 144128.0, \"curr\": 101888.0, \"time\": 0.0417180061340332}\n",
            "{\"granularity\": \"1*1*1*1*1 \", \"peak\": 145664.0, \"curr\": 91392.0, \"time\": 0.30886030197143555}\n",
            "{\"granularity\": \"1*10*2*1*1 \", \"peak\": 163328.0, \"curr\": 87552.0, \"time\": 0.28141069412231445}\n",
            "{\"granularity\": \"1*1*16*8*64 \", \"peak\": 4560158720.0, \"curr\": 1738558464.0, \"time\": 1.486714243888855}\n",
            "{\"granularity\": \"1*1*64*32*64 \", \"peak\": 5804918784.0, \"curr\": 341049088.0, \"time\": 2.677780508995056}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.remove(\"/content/drive/MyDrive/models/transformer/result.txt\")"
      ],
      "metadata": {
        "id": "xU3smUBZrwk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.experimental.reset_memory_stats('GPU:0')"
      ],
      "metadata": {
        "id": "Ph1KDdD08LMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1MksvcgKAffO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
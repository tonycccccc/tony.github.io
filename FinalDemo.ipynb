{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinalDemo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO8QWowahitubgW7J8IfVjP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tonycccccc/tony.github.io/blob/main/FinalDemo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8PilO4Tpt4X",
        "outputId": "75e66184-be4c-4854-9898-4153c6e78ff7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#import library\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import json\n",
        "#mount the google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#check the runtime type\n",
        "! nvidia-smi\n",
        "\"\"\"\n",
        "FLAT Dataflow Implementation:\n",
        "\"\"\"\n",
        "def FlatDataflow(loop_num, query, key, value, bias, batch_granularity_level1=1, batch_granularity_level2=1, head_granularity_level1=16,\n",
        "                 head_granularity_level2=8, length_granularity_level=64):\n",
        "  if (tf.config.list_physical_devices('GPU')):\n",
        "    memory_before = tf.config.experimental.get_memory_info('GPU:0')\n",
        "    print(\"Iteration %d: Before running, reset the memory! Memory peak: %f; Memory current: %f\"%(loop_num, memory_before['peak'], memory_before['current']))\n",
        "    batch_size, source_length, head_num, dim = tf.shape(query).numpy()\n",
        "    #since bias matrix shape is not fixed from time to time, randomly select a number here\n",
        "    #biasValue = bias[np.random.randint(tf.reshape(bias, [-1, 1]).size())]\n",
        "    bias_value = bias[0, 0, 0, 0]\n",
        "    for batch in tf.range(0, batch_size, batch_granularity_level1):\n",
        "      for head in tf.range(0, head_num, head_granularity_level1):\n",
        "        batch_termination = batch + batch_granularity_level1 if batch + batch_granularity_level1 <= batch_size else batch_size\n",
        "        for unit_batch in tf.range(batch, batch_termination, batch_granularity_level2):\n",
        "          if (batch_granularity_level2 != 1):\n",
        "            #Lowest granularity is batch level\n",
        "            end_batch = unit_batch + batch_granularity_level2 if unit_batch + batch_granularity_level2 <= batch_termination else batch_termination\n",
        "            query_source = tf.gather(query[:, :, :, :], indices=tf.range(unit_batch, end_batch), axis=0)\n",
        "            print(query_source.shape)\n",
        "            key_source = tf.gather(key[:, :, :, :], indices=tf.range(unit_batch, end_batch), axis=0)\n",
        "            value_source = tf.gather(value[:, :, :, :], indices=tf.range(unit_batch, end_batch), axis=0)\n",
        "            result = tf.einsum(\"BTNH, BFNH->BNFT\", key_source, query_source)\n",
        "            result += bias_value\n",
        "            result = tf.nn.softmax(result, name=\"attention_weights\")\n",
        "            result = tf.nn.dropout(result, rate=0.4)\n",
        "            attention_output = tf.einsum(\"BNFT,BTNH->BFNH\", result, value_source)\n",
        "          else:\n",
        "            head_termination = head + head_granularity_level1 if head + head_granularity_level1 <= head_num else head_num\n",
        "            for unit_head in tf.range(head, head_termination, batch_granularity_level2):\n",
        "              #Lowest granularity is head level\n",
        "              if (head_granularity_level2 != 1):\n",
        "                end_head = unit_head + head_granularity_level2 if unit_head + head_granularity_level2 <= head_termination else head_termination\n",
        "                query_source = tf.gather(query[unit_batch, :, :, :], indices=tf.range(unit_head, end_head), axis=1)\n",
        "                key_source = tf.gather(key[unit_batch, :, :, :], indices=tf.range(unit_head, end_head), axis=1)\n",
        "                value_source = tf.gather(value[unit_batch, :, :, :], indices=tf.range(unit_head, end_head), axis=1)\n",
        "                result = tf.einsum(\"TNH, FNH->NFT\", key_source, query_source)\n",
        "                result += bias_value\n",
        "                result = tf.nn.softmax(result, name=\"attention_weights\")\n",
        "                result = tf.nn.dropout(result, rate=0.4)\n",
        "                logit = tf.einsum(\"NFT,TNH->FNH\", result, value_source)\n",
        "                if unit_head == head:\n",
        "                  attention_output = logit\n",
        "                else:\n",
        "                  attention_output = tf.concat([attention_output, logit], axis=1)\n",
        "              else:\n",
        "                #Lowest granularity is length level\n",
        "                for length in tf.range(0, source_length, length_granularity_level):\n",
        "                  end_length = length + length_granularity_level if length + length_granularity_level <= source_length else source_length\n",
        "                  query_source = tf.gather(query[unit_batch, :, unit_head, :], indices=tf.range(length, end_length), axis=0)\n",
        "                  key_source = key[unit_batch, :, unit_head, :]\n",
        "                  result = tf.einsum(\"TH, FH->FT\", key_source, query_source)\n",
        "                  result += bias_value\n",
        "                  result = tf.nn.softmax(result, name=\"attention_weights\")\n",
        "                  result = tf.nn.dropout(result, rate=0.4)\n",
        "                  if length == 0:\n",
        "                    lengthOutput = result\n",
        "                  else:\n",
        "                    lengthOutput = tf.concat([lengthOutput, result], axis=0)\n",
        "                value_source = value[unit_batch, :, unit_head, :]\n",
        "                lengthRes = tf.einsum(\"FT,TH->FH\", lengthOutput, value_source)\n",
        "                lengthRes = tf.expand_dims(lengthRes, axis=1)\n",
        "                if (unit_head == head):\n",
        "                  attention_output = lengthRes\n",
        "                else:\n",
        "                  attention_output = tf.concat([attention_output, logit], axis=1)\n",
        "            attention_output = tf.expand_dims(attention_output, axis=0)\n",
        "          if unit_batch == batch:\n",
        "            output_from_unit_batch = attention_output\n",
        "          else:\n",
        "            output_from_unit_batch = tf.concat([output_from_unit_batch, attention_output], 0)\n",
        "        if head == 0:\n",
        "          suboutput = output_from_unit_batch\n",
        "        else:\n",
        "          suboutput = tf.concat([suboutput, output_from_unit_batch], 2)\n",
        "      if batch == 0:\n",
        "        output = suboutput\n",
        "      else:\n",
        "        output = tf.concat([output, suboutput], 0)\n",
        "    stoptime = time.time()\n",
        "    memory_after = tf.config.experimental.get_memory_info('GPU:0')\n",
        "    print(\"Iteration %d: After running! Memory peak: %f; Memory current: %f\"%(loop_num, memory_after['peak'], memory_after['current']))\n",
        "    return (memory_before['peak'], memory_before['current'], memory_after['peak'], memory_after['current'], stoptime)"
      ],
      "metadata": {
        "id": "-TmRp4QVwLRo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92d959cd-ef2a-49bb-c297-38631282233a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Dec 14 17:51:08 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    34W / 250W |    451MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#read the path to the target directory under user's google drive (modify the path /content/drive/MyDrive/path/to/ here before running)\n",
        "query_file = glob.glob(\"/content/drive/MyDrive/models/transformer/Test/logging_query2k.txt\")\n",
        "key_file = glob.glob(\"/content/drive/MyDrive/models/transformer/Test/logging_key2k.txt\")\n",
        "value_file = glob.glob(\"/content/drive/MyDrive/models/transformer/Test/logging_value2k.txt\")\n",
        "bias_file = glob.glob(\"/content/drive/MyDrive/models/transformer/Test/logging_bias2k.txt\")\n",
        "#read the file and parse it to tensor before sending to the flat function\n",
        "peak = []\n",
        "curr = []\n",
        "t = []\n",
        "#ask for user input\n",
        "batch1 = int(input(\"Enter Batch Granularity Level1:\"))\n",
        "batch2 = int(input(\"Enter Batch Granularity Level2:\"))\n",
        "head1 = int(input(\"Enter Head Granularity Level1:\"))\n",
        "head2 = int(input(\"Enter Head Granularity Level2:\"))\n",
        "length = int(input(\"Enter Length Granularity Level:\"))\n",
        "tf.config.experimental.reset_memory_stats('GPU:0')\n",
        "start_time = time.time()\n",
        "#write result to a temporary file\n",
        "path = \"/content/drive/MyDrive/models/transformer/result.txt\"\n",
        "f = open(path, 'a')\n",
        "grastr = \"%d*%d*%d*%d*%d \" % (batch1, batch2, head1, head2, length)\n",
        "for idx, file in enumerate(query_file):\n",
        "  query = tf.io.parse_tensor(tf.io.read_file(file), out_type=tf.float32)\n",
        "  key = tf.io.parse_tensor(tf.io.read_file(file), out_type=tf.float32)\n",
        "  value = tf.io.parse_tensor(tf.io.read_file(file), out_type=tf.float32)\n",
        "  bias = tf.io.parse_tensor(tf.io.read_file(file), out_type=tf.float32)\n",
        "  print(query.shape)\n",
        "  peakOld, currOld, peakCurr, currCurr, stoptime = FlatDataflow(idx, query, key, value, bias, batch_granularity_level1=batch1, batch_granularity_level2=batch2,\n",
        "                                                      head_granularity_level1=head1, head_granularity_level2=head2, length_granularity_level=length)\n",
        "  peak.append(peakCurr)\n",
        "  curr.append(currCurr)\n",
        "  t.append(stoptime - start_time)\n",
        "dir = {\"granularity\": grastr, \"peak\": sum(peak)/len(peak), \"curr\": sum(curr)/len(curr), \"time\": t[-1]/len(t)}\n",
        "f.write(json.dumps(dir))\n",
        "f.write(\"\\n\")\n",
        "f.close()\n",
        "iteration = np.arange(len(peak))\n",
        "plt.plot(iteration,peak,'r',label=\"Peak\")\n",
        "plt.plot(iteration,curr,'b',label='Current')\n",
        "plt.legend()\n",
        "plt.title(\"Peak&Current Memory Usage\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxBEDh_RsoWU",
        "outputId": "4ad21d1a-9115-46c6-9b6e-468d558cfc02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter Batch Granularity Level1:2\n",
            "Enter Batch Granularity Level2:2\n",
            "Enter Head Granularity Level1:2\n",
            "Enter Head Granularity Level2:2\n",
            "Enter Length Granularity Level:2\n",
            "(1, 262144, 16, 64)\n",
            "Iteration 0: Before running, reset the memory! Memory peak: 67328.000000; Memory current: 18176.000000\n",
            "(1, 262144, 16, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(path, 'r')\n",
        "file_contents = f.read()\n",
        "print (file_contents)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "1zv-DMw6QjTd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6bd7745-7064-4f5d-9f23-23902c815414"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"granularity\": \"1*1*1*1*32 \", \"peak\": 144640.0, \"curr\": 85760.0, \"time\": 0.707266092300415}\n",
            "{\"granularity\": \"1*1*1048*512*64 \", \"peak\": 111360.0, \"curr\": 85248.0, \"time\": 0.5663280487060547}\n",
            "{\"granularity\": \"16384*16384*16484*1*1 \", \"peak\": 166144.0, \"curr\": 133376.0, \"time\": 0.2881624698638916}\n",
            "{\"granularity\": \"1*1*4096*2048*64 \", \"peak\": 143616.0, \"curr\": 86016.0, \"time\": 0.6527509689331055}\n",
            "{\"granularity\": \"10*10*1*1*1 \", \"peak\": 112896.0, \"curr\": 88576.0, \"time\": 0.29667139053344727}\n",
            "{\"granularity\": \"10*10*1*1*1 \", \"peak\": 144128.0, \"curr\": 101888.0, \"time\": 0.0417180061340332}\n",
            "{\"granularity\": \"1*1*1*1*1 \", \"peak\": 145664.0, \"curr\": 91392.0, \"time\": 0.30886030197143555}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.remove(\"/content/drive/MyDrive/models/transformer/result.txt\")"
      ],
      "metadata": {
        "id": "xU3smUBZrwk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.experimental.reset_memory_stats('GPU:0')"
      ],
      "metadata": {
        "id": "Ph1KDdD08LMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1MksvcgKAffO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}